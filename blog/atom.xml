<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Kai Chen&#39;s Homepage</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://github.com/"/>
  <updated>2019-08-18T03:43:24.412Z</updated>
  <id>http://github.com/</id>
  
  <author>
    <name>Kai Chen</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>服务器使用经验汇总</title>
    <link href="http://github.com/server.html"/>
    <id>http://github.com/server.html</id>
    <published>2019-08-07T13:30:24.000Z</published>
    <updated>2019-08-18T03:43:24.412Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p>服务器使用jupyter notebook</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 服务器上运行</span></span><br><span class="line">jupyter notebook --no-browser --port=8889</span><br><span class="line"><span class="comment"># 本地运行</span></span><br><span class="line">ssh -N -f -L 127.0.0.1:8889:127.0.0.1:8889 username@address</span><br><span class="line"><span class="comment"># 本地打开 http://localhost:8889/token=...即可</span></span><br></pre></td></tr></table></figure></li><li><p>配置环境</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">conda create -n py3 python=3.6</span><br><span class="line"><span class="built_in">source</span> activate py3</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以将指令写在一个sh脚本当中就可以使用nohup后台安装packet</span></span><br><span class="line"><span class="comment"># test.sh</span></span><br><span class="line"><span class="built_in">echo</span> start</span><br><span class="line"><span class="keyword">while</span> <span class="built_in">read</span> requirement; <span class="keyword">do</span> conda install --yes <span class="variable">$requirement</span> || pip install <span class="variable">$requirement</span>; <span class="keyword">done</span> &lt; requirements.txt</span><br><span class="line"><span class="built_in">echo</span> finish</span><br><span class="line"><span class="comment"># run</span></span><br><span class="line">nohup ./test.sh &amp;</span><br></pre></td></tr></table></figure></li><li><p>手动指定conda env位置和pkt cache位置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在用户目录下面创建.condarc文件，并指定</span></span><br><span class="line">envs_dirs:</span><br><span class="line">  - /l/vision/v6/kaichen/anaconda/envs</span><br><span class="line">pkgs_dirs:</span><br><span class="line">  - /l/vision/v6/kaichen/anaconda/pkgs</span><br></pre></td></tr></table></figure></li><li><p>bash_profile</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 登陆的默认操作</span></span><br><span class="line">vim .bash_profile</span><br></pre></td></tr></table></figure></li><li><p>服务器下载并使用dropbox</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">$ wget https://raw.github.com/andreafabrizi/Dropbox-Uploader/master/dropbox_uploader.sh</span><br><span class="line">$ chmod +x dropbox_uploader.sh</span><br><span class="line"></span><br><span class="line">查询用户信息</span><br><span class="line">$ ./dropbox_uploader.sh info</span><br><span class="line">显示根目录内容</span><br><span class="line">$ ./dropbox_uploader.sh list</span><br><span class="line">要列出某个特定文件夹中的所有内容</span><br><span class="line">$ ./dropbox_uploader.sh list Documents/manuals</span><br><span class="line">上传一个本地文件到一个远程的 Dropbox 文件夹</span><br><span class="line">$ ./dropbox_uploader.sh upload snort.pdf Documents/manuals</span><br><span class="line">从 Dropbox 下载一个远程的文件到本地</span><br><span class="line">$ ./dropbox_uploader.sh download Documents/manuals/mysql.pdf ./mysql.pdf</span><br><span class="line">从 Dropbox 下载一个完整的远程文件夹到一个本地的文件夹</span><br><span class="line">$ ./dropbox_uploader.sh download Documents/manuals ./manuals</span><br><span class="line">在 Dropbox 上创建一个新的远程文件夹</span><br><span class="line">$ ./dropbox_uploader.sh mkdir Documents/whitepapers</span><br><span class="line">完全删除 Dropbox 中某个远程的文件夹（包括它所含的所有内容）</span><br><span class="line">$ ./dropbox_uploader.sh delete Documents/manuals</span><br></pre></td></tr></table></figure></li><li><p>screen</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 建立新的screen，打开一个新的bash</span></span><br><span class="line">$ screen</span><br><span class="line"><span class="comment"># ctrl + a + d 将screen置入后台运行，期间可以logout服务器</span></span><br><span class="line"></span><br><span class="line">$ screen -ls</span><br><span class="line">$ screen -r <span class="comment">#process_id</span></span><br></pre></td></tr></table></figure></li><li><p>python添加环境变量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">'your path'</span>)</span><br></pre></td></tr></table></figure></li><li><p>进程前后台切换</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ nohup python train.py &amp;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 运行程序时，先按ctrl z使其进入后台</span></span><br><span class="line"><span class="comment"># 2. 使用jobs查看对应的job 编号（和PID不同）</span></span><br><span class="line"><span class="built_in">jobs</span></span><br><span class="line"><span class="comment"># 3. fg是前台，bg是后台，但是只有nohup是忽略输出</span></span><br><span class="line"><span class="built_in">fg</span>/<span class="built_in">bg</span> %1</span><br></pre></td></tr></table></figure></li><li><p>指定GPU</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> CUDA_VISIBLE_DEVICES=ID</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;&lt;p&gt;服务器使用jupyter notebook&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span
      
    
    </summary>
    
      <category term="Computer Science" scheme="http://github.com/categories/Computer-Science/"/>
    
      <category term="Technique" scheme="http://github.com/categories/Computer-Science/Technique/"/>
    
    
      <category term="server" scheme="http://github.com/tags/server/"/>
    
  </entry>
  
  <entry>
    <title>AIR</title>
    <link href="http://github.com/AIR.html"/>
    <id>http://github.com/AIR.html</id>
    <published>2019-06-27T10:11:00.000Z</published>
    <updated>2019-06-27T10:12:52.847Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AIR"><a href="#AIR" class="headerlink" title="AIR"></a>AIR</h1><h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><ul><li><p>结构化的对图像image进行编码生成隐变量，相比于VAE &amp; DRAW等非结构化的方法来说对于downstream的泛化能力更强</p></li><li><p>infer the number of objects in the image推导得出图像中的object数量</p></li><li><p>describe every object with a latent variable对每一个object都使用一个隐向量进行特征描述，由于每幅图像的object个数不一定，所以AIR可以为每一幅图像生成一个<strong>变长的representation</strong>。</p></li><li><p>ST：spatial transfomer，只有加入了ST之后才可以赋予隐变量具体的实际意义，包括位置和形状，之后才能画出bounding box</p></li><li><p>p(n|θ)用一个几何分布，倾向于小数量的object</p></li></ul><h2 id="Idea"><a href="#Idea" class="headerlink" title="Idea"></a>Idea</h2><ul><li>难点：无监督——如何定义一个object &amp; 离散随机变量的求导</li><li>我们先sample图像中object的个数，再根据n确定sample的z（因为n不同，z的维度不同），再使用model来重构image x</li></ul><p>$$<br>p_\theta(x)=\sum_i^Np_N(n)\int p_\theta^z(z|n)p_\theta^x(x|z) dz<br>$$</p><ul><li><p>z = (z_what, z_where, z_pres)，其中z_pres是离散随机变量（几何分布 / 伯努利分布），z_where = [s, x, y]代表pose = scale &amp; position。z_what capture image语义信息，在AIR中即为object经过VAE的隐向量</p></li><li><p>使用RNN迭代，每一次只编码一个object</p></li><li><p>用来sample z的分布的参数都由inference net输出——sample操作对于离散和连续的随机变量都是不可导（这一点很好理解，因为你无法直观理解当参数改变Δw，结果会改变多少，<strong>公式理解就是将求导从期望运算E的外面移到E的里面</strong>）</p><ul><li><p>连续随机变量：重参数技巧，将sample过程单独移出来</p></li><li><p>离散随机变量：likelihood ratio estimator + neural baseline（减小梯度）</p><p>详细公式见<a href="http://pyro.ai/examples/svi_part_iii.html#Baselines-in-Pyro" target="_blank" rel="noopener">ELBO的求导</a></p></li></ul></li></ul><h2 id="How-it-work"><a href="#How-it-work" class="headerlink" title="How it work"></a>How it work</h2><ul><li><p>当前时步：使用RNN输出z_pres和z_where，若z_pres为0则前馈结束；将x和z_where输入到STN中，得到object的crop，使用VAE重构，其中VAE生成的隐变量表示即为z_what，重构object输入到STN中做之前的逆过程（即放大到原来image的size），每一时步重构image相加得到最终的重构图像</p><p><img src="images/AIR/1.png" alt="img"></p></li><li><p>因为一个batch中每张image可能有不同的object数量，所以我们不能简单的说当前出现z_pres == 0就break循环，但是我们必须阻止这个样本下的梯度传播，所以对于上述的每一个输出值都应该乘prev.z_pres来截断梯度</p></li><li><p>STN：spatial transfomer network。其实主要就是做一个仿射变换（原图像的一个放缩加上平移）<br>$$<br>\left[  </p><pre><code>\begin{matrix}        s &amp; 0 &amp; x \\     0 &amp; s &amp; y \end{matrix}</code></pre><p>\right] </p><ul><li>\left[<br>  \begin{matrix}    <pre><code>a \\ b \\ 1 </code></pre>  \end{matrix}<br>\right]<br>=<br>\left[<br>  \begin{matrix}    <pre><code>s*a+x \\ s*b+y </code></pre>  \end{matrix}<br>\right]<br>$$</li></ul></li><li><p>因为totallly unsupervised，所以我们必须加入机制来告诉model哪一个object已经看过，哪一个object没有看过，在实现的时候使用了以下trick：</p><ul><li>RNN的记忆功能</li><li>每次RNN的输入用上 <code>prev.z_what</code>, <code>prev.z_where</code>, <code>prev.z_pres</code>一起作为输入</li><li>DAIR：每个时步使用 <code>(x^i - x)</code>，即当前重构结果和原image的差作为RNN的输入</li></ul></li></ul><h2 id="source-code"><a href="#source-code" class="headerlink" title="source code"></a>source code</h2><p>​    其实实现上最大的困难应该就是z_pres如何将梯度传下去，这份pytorch源码中使用的是一个baseline_loss：网络预测值为伯努利的成功概率，baseline loss = infer_net的成功概率预测值 - baseline_net的成功概率预测值，这样跳过了sample过程，将梯度传下去了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        N = <span class="number">3</span></span><br><span class="line">        code = latent_code_state(x.size(<span class="number">0</span>), self.z_pres_dim, self.z_where_dim,</span><br><span class="line">                                 self.z_what_dim, self.lstm_state_dim)</span><br><span class="line">        recon_x = torch.zeros_like(x.view(<span class="number">-1</span>, <span class="number">2500</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(N):</span><br><span class="line">            code = self.infer_net(x.view(<span class="number">-1</span>, <span class="number">2500</span>), code)</span><br><span class="line">            </span><br><span class="line">            recon_x = self.generator_net(recon_x, code).view(<span class="number">-1</span>, <span class="number">2500</span>)</span><br><span class="line">       </span><br><span class="line">        kld_loss = compute_kld(code.z_what, code.z_what_mu, code.z_what_logvar) + \</span><br><span class="line">                   compute_kld(code.z_where, code.z_where_mu, code.z_where_logvar)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># baseline loss将梯度直接传到了z_pres_prob</span></span><br><span class="line">        baseline_net_loss = F.mse_loss(code.baseline_z_pres_prob, code.z_pres_prob)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> recon_x, code, kld_loss, baseline_net_loss</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, x)</span>:</span></span><br><span class="line">    <span class="comment"># 最终的loss值为：重构误差 + KL散度(z_what &amp; z_where) + baseline_loss</span></span><br><span class="line">    batch_loss += loss_function(recon_x, x) + kld.sum() + baseline_loss.sum()</span><br></pre></td></tr></table></figure><h2 id="问题："><a href="#问题：" class="headerlink" title="问题："></a>问题：</h2><ul><li><p>why sample而不是像有监督学习一样直接预测实际值？-&gt;更强的取值能力</p><p>-&gt;因为无监督嘛-&gt;都是N(0,1)</p><p>所以隐变量先验都是标准正态分布，将复杂性放到模型上</p></li><li><p>任务——scene understanding还是object detection-&gt;目标检测的话直接attention不久找到了吗？</p><p>无监督下只有通过重构才可以知道模型效果怎么样，否则无法训练</p></li><li><p>bounding box怎么画出来的？——z_where + STN吗？</p><p>图片中心偏移</p></li><li><p>为什么先验和似然都可以人为指定？因为高斯的拟合能力太强了吗？</p><p>只不过用高斯分布实验效果不错</p></li><li><p>两个estimator公式的区别在哪里？</p></li></ul><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="http://pyro.ai/examples/air.html" target="_blank" rel="noopener">超详细解析加代码</a></li><li><a href="http://akosiorek.github.io/ml/2017/09/03/implementing-air.html" target="_blank" rel="noopener">Deepmind小哥</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;AIR&quot;&gt;&lt;a href=&quot;#AIR&quot; class=&quot;headerlink&quot; title=&quot;AIR&quot;&gt;&lt;/a&gt;AIR&lt;/h1&gt;&lt;h2 id=&quot;特点&quot;&gt;&lt;a href=&quot;#特点&quot; class=&quot;headerlink&quot; title=&quot;特点&quot;&gt;&lt;/a&gt;特点&lt;/h2&gt;&lt;u
      
    
    </summary>
    
      <category term="Computer Science" scheme="http://github.com/categories/Computer-Science/"/>
    
      <category term="Computer Vision" scheme="http://github.com/categories/Computer-Science/Computer-Vision/"/>
    
    
      <category term="Unsupervised Object Detection" scheme="http://github.com/tags/Unsupervised-Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>梅陇镇首签——成功水过</title>
    <link href="http://github.com/first-us-visa.html"/>
    <id>http://github.com/first-us-visa.html</id>
    <published>2019-06-27T08:48:21.000Z</published>
    <updated>2019-06-27T10:03:29.380Z</updated>
    
    <content type="html"><![CDATA[<p>（本文写于6月27日飞往底特律的飞机上，希望能顺利到达Indy）</p><p>​    想写这篇总结文其实也已经很久了，但是因为签证时间正值期中，之后又有NLP beginner的4个task缠身导致一直没有办法完成。先说一下结果：我从4月份开始准备签证，预约2019年5月初在上面梅陇镇广场的美签中心面签，全程很顺利大概3个小时以内，在面前的第二天查到visa approved status<a href="https://ceac.state.gov/CEACStatTracker/Status.aspx?eQs=WwjqOlbeRYzCYubaSQI+RA==" target="_blank" rel="noopener">（查询网址）</a>并且在一周之内就收到了护照。</p><h1 id="DS160"><a href="#DS160" class="headerlink" title="DS160"></a>DS160</h1><p>​    DS160是美签统一的移民申请表，不管你申请何种形式的签证都需要填写，主要就是以身份信息和旅行信息为主，填写之前其实并不需要购买好机票。网上有很多DS160的填写教程这里就不再多说了，现在想来好像也没有什么很难回答的模棱两可的问题，但是当初还是填了好几天，可能是被拒签率给吓到了吧hhh。这里唯一有想提到的一些东西就是：</p><ul><li><p>照片：美签的照片真的是难以描述，首先照片的尺寸就和其他签证不同（600×600），这一点一定要注意。而且一定要有完整的正脸，包括头发和下巴都不能截取，否则无法使用，会被当场要求出签证中心重新拍照。</p><p>关于照片，有学长推荐可以到地铁的证件照拍摄机器来拍，我不知道靠谱不靠谱，反正我自己在地铁拍了一张美签证件照就自动把我的头发截掉了一点导致我面前的时候差点需要重拍，幸亏我多带了我几年前拍的照片（这里也可以看到实际上并没有严格的半年内拍摄要求，只要和你现在的长相差不多就好，但是在面前的时候你肯定还是要和他说是最近拍的）</p></li><li><p>就算有补助再填写的时候还是选自费。这是小蜜告诉我的专业意见，在面前的时候也没有被质疑到。</p></li><li><p>额外联系信息：不可写直系亲属，卖两个基友就好啦！</p><p>完成后会生成两个文件：一个是application form一个是confirmation form，其中申请表不用打印，面前的时候不会用到；confirmation form一定要带，是我们进入使馆的凭证。</p></li></ul><h1 id="DS2019-amp-DS7002"><a href="#DS2019-amp-DS7002" class="headerlink" title="DS2019 &amp; DS7002"></a>DS2019 &amp; DS7002</h1><p>​    这两个表是暑研对方学校发来的官方表格，也是J1签证的官方证明表格，<strong>非常重要</strong>！其中DS2019是J1学者的身份证明表，DS7002每个人不一定有，因为IUB提供给我补助，所以我在美多了一个student intern的身份，DS7002是证明这一身份的。这两个表面签，在美期间都一定要随身携带。</p><p>​    DS2019的办理需要学校方和美国的移民局之类的政府机构negotiation，很多教授并不愿意为暑研学生办理，这也就是为什么很多同学持B1旅游签证到美国暑研的原因，进而带来的就是高拒签率。所以在这里还是提醒大家：能不来美国就不来美国，能选官方项目就选官方项目。审心又审事。</p><h1 id="I-901"><a href="#I-901" class="headerlink" title="I-901"></a>I-901</h1><p>​    所有赴美学者/学生除了160刀的签证费，还需要交180刀的SEVIS FEE，美帝真的是想尽办法在捞钱。SEVIS FEE必须在面签的三天之前交掉，付款之后会有收据，即I-901表格，也是面签时候的一个重要材料。</p><h1 id="预约面签"><a href="#预约面签" class="headerlink" title="预约面签"></a>预约面签</h1><p>​    所有材料准备好了就可以到美国使领馆官网预约面签啦！在预约之前先要在官网注册一个账号，其中需要输入DS160的编号来匹配你的申请，也会根据你在填写DS160的时候所选择的签证使领馆位置来给出预约时间单。不得不承认的一点是，预约时间并不是相同的开放给不同的签证类型的，也就是说我作为J1签证申请者能够预约的时间明显是比B1的同学要多的 e.g.畅扬比我更早预约都只有6月份的签位，而我预约的时候最早有4月底的位置hhh（美帝的阶级概念是真的深入人心）。预约面签之后有一个预约确认信，需要打印面前当天使用。</p><h1 id="保险"><a href="#保险" class="headerlink" title="保险"></a>保险</h1><p>​    每个在美的J1签证者都需要为自己在美国的整段时间购买保险，且必须满足美国移民局制定的最低标准。网上也有很多此类保险机构，这里我购买的是ISO保险的J1保险，也是ISO这家公司的一个主营项目。在ISO官网上可以通过输入签证类型和学校来自动识别保险要求以购买达到要求的项目，价格算是我看到的几个公司当中最便宜的了每个月39刀。保险最低3个月。</p><h1 id="面签"><a href="#面签" class="headerlink" title="面签"></a>面签</h1><h2 id="材料"><a href="#材料" class="headerlink" title="材料"></a>材料</h2><p>​    首先在这里贴一下我在面签当天带去的材料，其中有很多都是我在之前英签的时候用到的。</p><ul><li><p>预约信打印件、DS160确认、照片纸质版（防止照片不行）、护照</p></li><li><p>DS2019 &amp; DS7002、I-901</p></li><li><p>资产证明</p></li><li><p>邀请信、 实验室网页、国内外导师网页介绍</p></li><li><p>托福成绩单、雅思成绩单 ，Study Plan（官方有模板）</p></li><li><p>自己的CV（官方有模板）、教授CV</p></li><li><p>学生证、在读证明、成绩单</p><p>其中真正用到的只有：DS2019以及各种确认信。如果面签官除了你的护照没有留下任何材料，恭喜你面签通过；如果面签官留下了你的护照和材料，恭喜你你被check了，即所谓的行政审查，往往是因为设计敏感专业（e.g. CS），check时间很玄学，短的只要一个月，长的半年的也有；如果你的护照都没有留下，那么你被拒签了= =，准备重新来过吧，但是其实只要理由正确，不是B1去暑研的，一般都不会被拒签。</p></li></ul><h2 id="当天流程"><a href="#当天流程" class="headerlink" title="当天流程"></a>当天流程</h2><ul><li>一早到梅陇镇，先去隔壁存包，感觉还是很靠谱的，虽然规定不能带包但是还是几乎每个人都带了包</li><li>排队进馆、安检、录指纹、录照片，整个过程还是很漫长的，占据了整个面签过程的大部分时间。当然只要材料准备的好，一般都没什么问题，大家可以多和身边的好看小姐姐交流一下，我在排队的时候就从一个大二的小妹妹那边知道了一些对我来说是新世界的东西hhh。</li><li>排队面签——面签官有男有女，亚洲人、白人和黑人都有。一亩三分地给出的情况是黑人男&gt;黑人女&gt;白人男&gt;白人女&gt;亚裔男&gt;亚裔女，不好说准不准确吧，但是梅陇镇的一些白人面签官确实看上去都挺和蔼可亲的，小姐姐小哥哥笑得都很开心让人很放松，相比之下亚洲面签官就显得比较严肃和认真了。</li><li><p>给我面签的是一个白人小哥（面我之前刚刚check了一个biology MS，搞得我很慌）：</p><ul><li>我：Morning, Sir! I’m applying for a J1 visa.</li><li>VO: Morning! Please give me your passport and DS2019 please.</li><li>（一长段沉默和录入信息的打字声）</li><li>VO: So you are a Computer Science student.</li><li>我: Yeah（尴尬的笑笑）</li><li>VO: Undergraduate? Junior?</li><li>我：Yes</li><li>VO: So what’s your research topic?</li><li>我：（大白话讲解object detection，事先准备）这个过程千万不能提及任何敏感词汇，而且要尽量解释的详细易懂，越easy越能赢得面签官的信任！</li><li>VO: Congratulations, your visa has been approved! （顺便提醒了以下蓝色的维权小册子）</li><li>我：Thank you very much!</li></ul></li><li><p>网上说的蓝条通过现在已经没有了、黄条红条check还是有的，会告知你已经提交的材料和可以补交的材料。</p></li><li>通过之后从出口出来，取包回张江！</li></ul><h2 id="一些很优秀的参考攻略"><a href="#一些很优秀的参考攻略" class="headerlink" title="一些很优秀的参考攻略"></a>一些很优秀的参考攻略</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/20801690" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/20801690</a></li><li><a href="http://www.k1u.com/trip/24275.html" target="_blank" rel="noopener">http://www.k1u.com/trip/24275.html</a></li><li><a href="https://www.cmwonderland.com/2018/05/15/J-1-Visa/" target="_blank" rel="noopener">https://www.cmwonderland.com/2018/05/15/J-1-Visa/</a></li></ul><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>​    整个流程还是蛮简单轻松的，只要有合理的依据和对应的证明文件实际上拒签率是很低的，大家担心的问题主要还是check，但是我感觉其实只要放轻松的话J1这种短期签证实际上还好，check的主要受灾户还是MS和PHD的朋友们（毕竟他们才是真正的高端知识人群hhh）。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;（本文写于6月27日飞往底特律的飞机上，希望能顺利到达Indy）&lt;/p&gt;
&lt;p&gt;​    想写这篇总结文其实也已经很久了，但是因为签证时间正值期中，之后又有NLP beginner的4个task缠身导致一直没有办法完成。先说一下结果：我从4月份开始准备签证，预约2019年5
      
    
    </summary>
    
      <category term="出国" scheme="http://github.com/categories/%E5%87%BA%E5%9B%BD/"/>
    
      <category term="visa" scheme="http://github.com/categories/%E5%87%BA%E5%9B%BD/visa/"/>
    
    
      <category term="梅陇镇" scheme="http://github.com/tags/%E6%A2%85%E9%99%87%E9%95%87/"/>
    
      <category term="经验总结" scheme="http://github.com/tags/%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>intern</title>
    <link href="http://github.com/intern.html"/>
    <id>http://github.com/intern.html</id>
    <published>2019-05-25T12:09:53.000Z</published>
    <updated>2019-06-27T10:07:18.905Z</updated>
    
    <content type="html"><![CDATA[<p>​    立这一帖主要还是因为自己太菜了，每一次面试的时候总会遇上自己不懂的题。</p><h1 id="程序基础"><a href="#程序基础" class="headerlink" title="程序基础"></a>程序基础</h1><ul><li><p>C++的虚函数和纯虚函数的区别？</p><ul><li><p>C++中只有虚函数（加了virtual标志）才可以从父类中继承并且覆盖，否则就算子类中有何父类相同名相同参数的函数也会被忽视而执行父类中的函数。</p></li><li><p>纯虚函数指的是抽象类的虚函数，抽象类在定义class前面加virtual</p></li><li><p>抽象类不可以实例化，而不仅仅是没有意义</p></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span>&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">foo</span><span class="params">()</span></span>&#123;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"1\n"</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">fun</span><span class="params">()</span></span>&#123;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"2\n"</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">B</span> :</span> <span class="keyword">public</span> A&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">foo</span><span class="params">()</span></span>&#123;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"3\n"</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">fun</span><span class="params">()</span></span>&#123;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"4\n"</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">void</span>)</span></span>&#123;</span><br><span class="line">A a;</span><br><span class="line">B b;</span><br><span class="line">A *p = &amp;a;</span><br><span class="line">p-&gt;foo();</span><br><span class="line">p-&gt;fun();</span><br><span class="line">p = &amp;b;</span><br><span class="line">p-&gt;foo();</span><br><span class="line">p-&gt;fun();</span><br><span class="line">&#125; <span class="comment">// 输出1214</span></span><br></pre></td></tr></table></figure></li><li><p>引用和指针的区别？</p><ul><li>指针本身是一个变量，是一个64bit = 8字节的变量</li><li>引用就是变量的一个别名，调用的方式相同</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> a;</span><br><span class="line"><span class="keyword">int</span>* b = &amp;a;</span><br><span class="line"><span class="keyword">int</span> &amp;c = a;</span><br></pre></td></tr></table></figure></li></ul><h1 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h1><ul><li><p>一个链表判断是否有环？环的长度？环的开始？</p><ul><li>快慢指针</li><li>环的开始：假设慢指针走了a+x，快指针走了a+mb+x，其中a是入口距离，b是环的长度，x是相遇点的offset，因为2(a+x) = a+mb+x可以得到a = mb - x。所以只需要两个指针一个从相遇点一个从链表头出发一定会在入口相遇</li><li>环上任意一点开始，走一圈都原来的地方就是环的长度</li></ul></li><li><p>最大子矩阵和O(n^3)</p><p>dp{m}{n}{t}，枚举两行m和n，然后把问题退化为一维的最大子序和问题</p><p><img src="/images/intern/1.png" alt="img"></p></li><li><p>给定一个多项式，已知<br>$$<br>f(x) = a_0 + a_1x+…+a_nx^n<br>$$</p><ul><li>ai都是非负整数</li><li>n不知道多大</li><li>唯一的操作：输入x，输出f(x)</li><li>我要知道每一个ai</li><li>如果ai有界，我们只需要带入一个比所有ai都大值m，再把输出转化为m进制表示即可。现在不知道ai上界，我们先令x=1得到ai的上界s，把s+1能保证这个值大于每一个ai，把s+1代入即可。</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;​    立这一帖主要还是因为自己太菜了，每一次面试的时候总会遇上自己不懂的题。&lt;/p&gt;
&lt;h1 id=&quot;程序基础&quot;&gt;&lt;a href=&quot;#程序基础&quot; class=&quot;headerlink&quot; title=&quot;程序基础&quot;&gt;&lt;/a&gt;程序基础&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;C++的虚
      
    
    </summary>
    
      <category term="Computer Science" scheme="http://github.com/categories/Computer-Science/"/>
    
    
      <category term="intern" scheme="http://github.com/tags/intern/"/>
    
  </entry>
  
  <entry>
    <title>VAE &amp; GANS</title>
    <link href="http://github.com/VAE_GANS.html"/>
    <id>http://github.com/VAE_GANS.html</id>
    <published>2019-02-05T14:34:11.000Z</published>
    <updated>2019-06-27T10:09:51.374Z</updated>
    
    <content type="html"><![CDATA[<h1 id="VAE-amp-GANS"><a href="#VAE-amp-GANS" class="headerlink" title="VAE &amp; GANS"></a>VAE &amp; GANS</h1><h2 id="AutoEncoder"><a href="#AutoEncoder" class="headerlink" title="AutoEncoder"></a>AutoEncoder</h2><p>自动编码器（AE）是VAE的基础，在这里不多赘述，多用做一种<strong>降维</strong>的方式。由高维输入x经过一个神经网络<strong>Encoder</strong>得到一个低维隐向量z，再由隐向量z通过另一个神经网络<strong>Decoder</strong>还原为高位向量x。</p><p>VAE是在AE中引入了随机性，从而使得VAE在得以拥有高还原效果的同时得到生成新样本的能力。</p><h2 id="VAE"><a href="#VAE" class="headerlink" title="VAE"></a>VAE</h2><ul><li><p>有数据集{xi}i=1…N，想要得到x的真实分布p(x)</p></li><li><p>我们尝试用隐变量，其中隐变量满足一个普通的高斯分布（先验分布设成高斯很正常，否则为什么要 用隐变量= =）。然后希望学习<strong>一个分布之间的变换</strong>，使得我们可以使用一个普通的先验分布<strong>p(z)</strong>(e.g. 正态分布)变换得到分布<strong>P’(x)</strong>使得其与真是分布<strong>P(x)</strong>尽可能相同。生成器我们可以使用神经网络进行拟合。</p></li><li><p>问题是我们只有两个分布的采样，没有他们的表达式就没有办法使用KL散度来衡量两个分布之间的差异</p><p><img src="images/VAE/4.jpg" alt="img"></p></li></ul><h3 id="Problem：不可积分"><a href="#Problem：不可积分" class="headerlink" title="Problem：不可积分"></a>Problem：不可积分</h3><p><img src="images/VAE/2.png" alt="img"></p><p>和多数生成模型相同，VAE同样对<strong>P(x)</strong>进行建模。一种方法如上，这种方法对于大数据集很难使用，因为P(X)不可积分，我们很难对每一个可能的z求积分，进而导致我们没有办法对后验概率建模，which is our encoder network。</p><p>所以我们使用一个Encoder network的variational inference：<strong>用q(Φ, z|x)对真实的P(θ, z|x)后验概率</strong>进行拟合估计之后使用贝叶斯公式可以得到P(X)</p><h3 id="Random-Factor"><a href="#Random-Factor" class="headerlink" title="Random Factor"></a>Random Factor</h3><p>为了生成新的模型，我们在模型中加入随机因素，Encoder Network只输出z符合的正态分布的均值和方差<strong>（假设先验分布为正态分布）</strong>，这样每次我们对这个分布进行采样解码就可以生成不同但是相似的样本。</p><h3 id="公式解析"><a href="#公式解析" class="headerlink" title="公式解析"></a>公式解析</h3><p>第一步公式变化见下面的图片（选自CS231n 生成模型PPT）</p><p><img src="images/VAE/1.png" alt="img"></p><ul><li><p>化简为一个Variational lower bound（代表我们一个拟合的下界）和一个KL散度（衡量两个后验分布之间的差异）。其中ELBO的第一部分代表对于隐向量的高维重建，这部分和AE类似，优化方法是比较重构结果和原始样本点的L2损失；</p></li><li><p>由于每个p(x | z)都是高斯分布，实际上我们的VAE也是一种<strong>（infinite的）高斯混合模型</strong>。模型主要的复杂能力来自于这个积分.<br>$$<br>p_\theta(x) =\int p_\theta(x | z)p_\theta(z) dz<br>$$</p></li><li><p>decoder神经网络的输出实际上就是L的第一项，即就是一个期望值。这是因为对于decoder来说，我们只是为了其衡量效果，对于方差我们一方面希望它尽量小，另一方面也并不希望它发生变动，所以这里我们假设p(x|z,θ)服从正态分布（是什么分布不重要，只要满足x(i)的对应概率高就好了）且其<strong>方差</strong>不变，这样我们的decoder network就只需要专心学习分布的均值就好了，最终通过公式推到就得到了和AE类似的MSE reconstruction loss。</p><p><img src="images/VAE/5.png" alt="img"></p></li><li><p>重参数技巧（reparameteration trick），因为我们在网络中输出μ和σ后对z进行了sample，但是如果直接使用σ进行采样的话会导致我们梯度的方差也会很大（有一个方差因子σ），会导致我们的训练过程波动很大，从而影响训练效果。因此我们必须将采样过程单独分离出来，即将μ和σ作为两个参数，而仅需在N(0, 1)上采样即可。</p><p><img src="https://pic1.zhimg.com/80/v2-39d484abe79242a398d6f57ee3d7dc04_hd.jpg" alt="img"></p></li><li><p>第二部分则是一个KL散度，说明我们希望将每一个qφ(z|x) 尽可能地和正态分布相同，这也就是训练时的一个loss来源。这符合我们最后先从标准正态分布中sample z，再用z重构x。</p></li></ul><p><img src="https://pic3.zhimg.com/80/v2-a3f264a40db57e010b7ebf0253198726_hd.jpg" alt="img"></p><ul><li><p><strong>Decoder Loss</strong>：右边第一项实际上是reconstruction的过程，要求重生成的图像和原图像相似，使用MSE loss可以，但是需要选定合适的超参数平衡两部分误差。</p></li><li><p><strong>Encoder Loss</strong>：右边第二项又是一个KL散度，这部分的优化需要一个trick：reparameter，具体过程见外链，简单来说就是由于我们认为P(z)服从一个一个简单的先验分布——（单位）正态分布，这样我们就可以根据KL的公式去得到一个只跟q(z)相关的loss，代表两个分布之间的差值，且直接和mean和variance相关。</p><p><img src="https://pic2.zhimg.com/80/v2-08091fc4fa1460c9fa611cfcf0608105_hd.jpg" alt="img"></p></li></ul><h3 id="模型分析"><a href="#模型分析" class="headerlink" title="模型分析"></a>模型分析</h3><ul><li><p>VAE的强大之处：encoder结果实际上只是高斯分布的一个均值，但是它可以通过加入单位化方差来加入高斯噪声，从而使得一个fixed mean vector得以包含更多的信息，再通过一个（类AE）的decoder network将这个信息回复出来，同时还实现了随机性。</p></li><li><p>这里我们将目标的先验分布设置成标准的正态分布有两个原因：<strong>均值为0，这是重构的要求</strong>，若方差也为0那么就退化成了AE，所以方差不为0，就相当于人为的引入了噪声，一方面是保持结果随机性，另一方面是为了增强decoder的鲁棒性。</p><p>重构是希望没有噪声的，而KL loss将分布趋向于标准正态分布实际上是倾向于引入噪声的，这样实际上也是一个对抗的过程。</p><p>Paperweekly的观点：</p><blockquote><p><strong>它本质上就是在我们常规的自编码器的基础上，对 encoder 的结果（在VAE中对应着计算均值的网络）加上了“高斯噪声”，使得结果 decoder 能够对噪声有鲁棒性；而那个额外的 KL loss（目的是让均值为 0，方差为 1），事实上就是相当于对 encoder 的一个正则项，希望 encoder 出来的东西均有零均值。</strong></p><p>那另外一个 encoder（对应着计算方差的网络）的作用呢？它是用来<strong>动态调节噪声的强度</strong>的。</p><p>直觉上来想，<strong>当 decoder 还没有训练好时（重构误差远大于 KL loss），就会适当降低噪声（KL loss 增加），使得拟合起来容易一些（重构误差开始下降）</strong>。</p><p>反之，<strong>如果 decoder 训练得还不错时（重构误差小于 KL loss），这时候噪声就会增加（KL loss 减少），使得拟合更加困难了（重构误差又开始增加），这时候 decoder 就要想办法提高它的生成能力了</strong>。</p><p>说白了，<strong>重构的过程是希望没噪声的，而 KL loss 则希望有高斯噪声的，两者是对立的。所以，VAE 跟 GAN 一样，内部其实是包含了一个对抗的过程，只不过它们两者是混合起来，共同进化的</strong>。</p></blockquote></li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/34998569" target="_blank" rel="noopener">最优秀的VAE解析</a></li><li><a href="https://blog.csdn.net/heyc861221/article/details/80130968" target="_blank" rel="noopener">CSDN的VAE&amp;GANS解析</a></li><li><a href="https://github.com/hwalsuklee/tensorflow-generative-model-collections/blob/master/README.md">各种VAE + GANS</a></li><li><a href="https://github.com/hindupuravinash/the-gan-zoo">GAN ZOO</a></li><li><a href="https://github.com/soumith/ganhacks">Github How to train GANs</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;VAE-amp-GANS&quot;&gt;&lt;a href=&quot;#VAE-amp-GANS&quot; class=&quot;headerlink&quot; title=&quot;VAE &amp;amp; GANS&quot;&gt;&lt;/a&gt;VAE &amp;amp; GANS&lt;/h1&gt;&lt;h2 id=&quot;AutoEncoder&quot;&gt;&lt;a href=
      
    
    </summary>
    
      <category term="Computer Science" scheme="http://github.com/categories/Computer-Science/"/>
    
      <category term="Computer Vision" scheme="http://github.com/categories/Computer-Science/Computer-Vision/"/>
    
    
      <category term="Generative Model" scheme="http://github.com/tags/Generative-Model/"/>
    
  </entry>
  
  <entry>
    <title>各种留学讲座Note</title>
    <link href="http://github.com/abroad-tips.html"/>
    <id>http://github.com/abroad-tips.html</id>
    <published>2019-02-02T13:49:11.000Z</published>
    <updated>2019-06-27T10:01:51.193Z</updated>
    
    <content type="html"><![CDATA[<h1 id="交大CMU-PHD"><a href="#交大CMU-PHD" class="headerlink" title="交大CMU PHD"></a>交大CMU PHD</h1><ul><li>三月份开始找暑研</li><li>如何准备idea回复：你为什么想找这个教授合作，你会什么，你之后想做什么</li><li>在发邮件联系自己感兴趣的教授的过程中，除了常规的介绍自己的科研经历以外，很重要的一点是能够理解教授真正感兴趣的点在哪里，讲清楚自己的兴趣是什么，自己的兴趣，技能和教授的兴趣如何能够结合起来，如果时间富裕的话，在这基础上再提出一些idea。在暑期科研的过程中，最重要的是让教授看出两点，科研热情和聪明（基础+上升空间）。而聪明某种程度上是可以通过不断积极去找人交流，看论文，做实验，不断积累而表现出来的。</li></ul><h1 id="GRE-amp-TOFEL"><a href="#GRE-amp-TOFEL" class="headerlink" title="GRE &amp; TOFEL"></a>GRE &amp; TOFEL</h1><p><img src="/images/Tips/1.jpg" alt="img"></p><p><img src="/images/Tips/2.jpg" alt="img"></p><p><img src="/images/Tips/3.jpg" alt="img"></p><p><img src="/images/Tips/4.jpg" alt="img"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;交大CMU-PHD&quot;&gt;&lt;a href=&quot;#交大CMU-PHD&quot; class=&quot;headerlink&quot; title=&quot;交大CMU PHD&quot;&gt;&lt;/a&gt;交大CMU PHD&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;三月份开始找暑研&lt;/li&gt;
&lt;li&gt;如何准备idea回复：你为什么想找这
      
    
    </summary>
    
      <category term="出国" scheme="http://github.com/categories/%E5%87%BA%E5%9B%BD/"/>
    
    
      <category term="note" scheme="http://github.com/tags/note/"/>
    
  </entry>
  
  <entry>
    <title>Manchester交流小结</title>
    <link href="http://github.com/Manchester.html"/>
    <id>http://github.com/Manchester.html</id>
    <published>2019-02-02T13:43:55.000Z</published>
    <updated>2019-02-10T09:52:32.727Z</updated>
    
    <content type="html"><![CDATA[<p>​    2018年的秋天，我作为交换生在英国曼彻斯特大学计算机学院度过了一个难忘的交流学期。希望借由这篇小结能够对这次的交流做一个总结并对之后到曼大交流的同学可以所有帮助。</p><h1 id="曼大学习"><a href="#曼大学习" class="headerlink" title="曼大学习"></a>曼大学习</h1><ul><li><p>英语要求，TOFEL和IELTS均可。</p><p>曼大的每一门课都对同学的英语成绩有所要求，即使是同一个学院的专业课也会有不同。在选课的时候选课老师会先根据课程要求判断你的英语成绩是否达标。在这一点的检查上和大多数的英国大学都是一样的严格，有的甚至对听说读写的小分都有要求。语言成绩要求可以在官网上查到。</p></li><li><p>Computer Science School</p><p>曼大要求每个交流生选择50-60学分的课程（每门课10-20学分），我一共选了5门课。幸运的是，CS的课程安排在不同的学校都很类似，因此可以比较准确的找到转学分的课程，但是在确认选课之后最好还是和复旦的老师确认一下学分转换的事情。</p><p>我选择的5门课分别是：Probability Ⅰ、Operating System、Fundamentals of Database、Machine Learning &amp; Optimization、Cryptography &amp; Network Security。都是很常见的计算机专业课，正因此让我对于英国和国内的教学质量和理念有了一个强烈的对比感：</p><ul><li><p>课程形式绝大多数均为’lecture+lab’的形式：</p><p>授课形式上并没有太大的差别，最大的差别可能就是曼大的每一门课都要求必须有lab课而且保证提供1周1个小时以上的上机时间。考虑到曼大计算机学院庞大的学生人数确实难能可贵了。</p></li><li><p>课程量和课程难度的降低。</p><p>在曼大上课的一个最大的感受就是课程涉及的内容确实难度不如国内对应课程的难度要大，而且所涉及到的知识量也会小很多。我觉得这一个是因为曼大一个学期只有12个教学周（其中第6周是reading weak不用上课），有限的教学时间让老师在课程设置的时候刻意的控制的教学量，很多课程甚至需要lecture以外的专门时间来进行答疑或者是测试；</p><p>另一方面我觉得是因为大多数的CS课程设置都以应用为主，而并不对理论背景，尤其是数学推理的过程做过多的要求。一个很明显的例子就是Machine Learning &amp; Optimization这门课，这是曼大的本科生ML入门课，在讲到SVM支持向量机的时候就完全的使用的是几何的解释方式，而将数学上的证明和推导全都放在了Optional Reading中。不可置疑的是这种方法能够在有限的时间内帮助同学更好的理解SVM的优化目标，但是这种偏向于engineer而非scientist的教学方式，在一定程度上体现了国内外至少在本科阶段的一个教学的差异。</p><p>再举一个可能CSer会更熟悉的例子：OS这门课在国内几乎可以使用灾难形容的，不仅仅有大量难懂的基础知识，更可怕的是附带的lab，一周可能要花上八个小时左右的时间，但是在曼大OS的lab一周可能只需要一两个小时，lab设置上更注重说给你写好一个大问题让你阅读代码读透代码中的trick，让我们对OS机制有更直观的理解，具体的实现细节并不是课程重点，这一点在最后的期末考试中也可以看出来。</p></li><li><p>Make Sense就好：这里以Fundamentals of Database这门课为例。数据库是一门和实际生活联系比较多的课程，在lab中需要我们根据给出的信息设计数据库结构。但是实际上在答题过程中会有很多不严谨的细节脑洞，每次询问TA都会得到类似的回答：你写个注释在旁边，说的有道理就好了。其实这种情况在期末考试中也很经常出现，老师提供的期末考试feedback（类似于参考答案）中往往不会有过多的答案限制。</p></li></ul></li><li><p>硬件方面如上所述，学生机房加起来就有足足一整层，几乎任何工作时间你都可以到机房使用学校机器。和大部分国外高校一样曼大使用 (Scientific) Linux系统，可以使用SSH远程登陆，摆脱虚拟机的困扰。令人惊奇的是Piazza在曼大并不普及，曼大使用自己的在线课程系统blackboard（和elearning类似），这同时也是曼大机考在线测试所使用的平台。</p></li></ul><h1 id="曼大生活"><a href="#曼大生活" class="headerlink" title="曼大生活"></a>曼大生活</h1><ul><li><p>住在曼大</p><p>初到曼城时已经是九月份的中旬，就已经刮起了秋风；离开的时候曼城则刚下了今年的初雪，虽然并不是很大，但是对于一个南方的孩子来说就已经可以说是欣喜若狂了。当然最可恨的还是我们一离开英国曼城就下了一场异常好看的大雪。曼城是典型的温带大陆性气候，入秋很早但冬天也并不是很冷，虽然下大雪但是大部分时间气温仍然保持在0度以上，和上海的冬天比起来可以说是温和许多了。</p><p><img src="\images\Manchester\1.JPG" alt="img"></p><p>我们并没有在曼城租房，而是选择了申请较为方便的学校宿舍。曼大并不像复旦校园一样用围栏围出一个小社区，曼大校园实际上是融合在了曼城的街道当中的，教学楼的分布仍然密集，但是南北跨度极长，分为南北两个校区，其中北校区主要是各种工科专业而南校区主要是理科和文科专业，南北校区距离大概是20-30分钟的路程。</p><p>我们一行人来自不同的学院，因此也就选择住在了不同的宿舍。在这里特别推荐北边靠近市中心的<a href="https://www.agoda.com/zh-cn/weston-hall-halls-of-residence/hotel/manchester-gb.html?cid=-38" target="_blank" rel="noopener">Weston Hall</a>，一个是因为地理位置特别好，不管是向北到市中心采购逛街，还是到南校区上课都只需要15分钟的路程，离曼城的几个中超、汽车站、火车站都很近；另一个是因为Weston hall是独立房间卫浴，共享厨房和公共空间的设置，而且设施都挺新的，定期还有人来打扫公共区域，是性价比很高的一个选择。</p></li></ul><ul><li><p>文化交流</p><p>在曼城一个学期能真切地感受到曼城不仅仅是中国人特别多，也是吸引了来自全世界各地的学生，以我自己的套间为例，我的舍友有两个英格兰人，一个法国小哥，一个西班牙小姐姐，一个巴基斯坦人，一个英籍印度人和一个中国人。虽然大家来自不同的地方，但是大家的相处都很愉快，在学生中并没有所谓的文化歧视的现象存在。</p></li></ul><ul><li><p>玩在曼城</p><p>曼城作为一个传统的工业城市，确实保留着很强的近现代气息，这一点在去过爱丁堡之后会有更强烈的对比。曼城的博物馆，美术馆，大小教堂都是人气很高的游玩去处，而且大部分都不需要购买门票。</p><p>说到曼城，最著名的当属它的足球文化。坐拥两支世界级的足球强队，一支在东，一支在西，曼城是红还是蓝一直是曼城人民争论的话题。曼城市中心还有一个足球博物馆，记载了英国近代足球的发展历史，博物馆内还有一个货真价实的英超奖杯，可以近距离的触摸，那个感觉厉害炸了。</p><p>我在reading weak到曼城主场Etihad Stadium看了一场比赛，虽然只是和富勒姆的联赛杯比赛，但是曼城球迷的热情却丝毫不减。孔帕尼的每次成功防守都能得到全场’Come On City’的呼声，德布劳内的每次传中都能让你屏气凝神心头一紧，那个晚上迪亚兹连进两球锁定胜局。比较遗憾的是没有到老特拉福德球场看一场曼联的球赛，曼大的International Society会组织参观球场的活动，有兴趣的话可以参加。球票的话曼大的SU和当地的中国学生会都会有定期的学生活动。</p><p>此外就是曼城在特殊节日比如万圣节、圣诞节、跨年夜都会在市政厅举办独特的庆祝活动，人山人海之时，更能让人感受到在异国的节日魅力。</p><p><img src="\images\Manchester\2.JPG" alt="img"></p></li></ul><p>​    在曼大的一个学期虽然短暂，回国之后却经常会回想起当时的点点滴滴。那种置于异国他乡独立生活的成就感将会是我一生的财富。感谢学校的交流机会，丰富了我的阅历和见识。希望自己在接下里的学习和生活中能够继续努力前行，今日我为复旦骄傲，明日复旦为我自豪！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;​    2018年的秋天，我作为交换生在英国曼彻斯特大学计算机学院度过了一个难忘的交流学期。希望借由这篇小结能够对这次的交流做一个总结并对之后到曼大交流的同学可以所有帮助。&lt;/p&gt;
&lt;h1 id=&quot;曼大学习&quot;&gt;&lt;a href=&quot;#曼大学习&quot; class=&quot;headerli
      
    
    </summary>
    
      <category term="Life" scheme="http://github.com/categories/Life/"/>
    
    
      <category term="Manchester" scheme="http://github.com/tags/Manchester/"/>
    
      <category term="交流小结" scheme="http://github.com/tags/%E4%BA%A4%E6%B5%81%E5%B0%8F%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch Notes</title>
    <link href="http://github.com/pytorch.html"/>
    <id>http://github.com/pytorch.html</id>
    <published>2019-02-01T08:31:13.000Z</published>
    <updated>2019-02-02T13:43:57.770Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><h1 id="Dataloader"><a href="#Dataloader" class="headerlink" title="Dataloader"></a>Dataloader</h1><p>首先是pytorch的导入数据API，这部分API我觉得整体思想和<code>tf.data</code>非常类似，都是将数据集合迭代器搭配使用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader <span class="comment">#loader库</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> sampler  <span class="comment"># 不是很重要的取样API</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torchvision.datasets <span class="keyword">as</span> dset <span class="comment"># 从dataset中生成loader</span></span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> T <span class="comment"># dataset的对应预处理操作</span></span><br><span class="line"></span><br><span class="line">transform = T.Compose([</span><br><span class="line">                T.ToTensor(),</span><br><span class="line">                T.Normalize((<span class="number">0.4914</span>, <span class="number">0.4822</span>, <span class="number">0.4465</span>), (<span class="number">0.2023</span>, <span class="number">0.1994</span>, <span class="number">0.2010</span>))</span><br><span class="line">            ])</span><br><span class="line"></span><br><span class="line">cifar10_train = dset.CIFAR10(<span class="string">'./cs231n/datasets'</span>, train=<span class="keyword">True</span>, download=<span class="keyword">True</span>,</span><br><span class="line">                             transform=transform)</span><br><span class="line">loader_train = DataLoader(cifar10_train, batch_size=<span class="number">64</span>, </span><br><span class="line">                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))</span><br><span class="line"></span><br><span class="line">cifar10_val = dset.CIFAR10(<span class="string">'./cs231n/datasets'</span>, train=<span class="keyword">True</span>, download=<span class="keyword">True</span>,</span><br><span class="line">                           transform=transform)</span><br><span class="line">loader_val = DataLoader(cifar10_val, batch_size=<span class="number">64</span>, </span><br><span class="line">                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, <span class="number">50000</span>)))</span><br><span class="line"></span><br><span class="line">cifar10_test = dset.CIFAR10(<span class="string">'./cs231n/datasets'</span>, train=<span class="keyword">False</span>, download=<span class="keyword">True</span>, </span><br><span class="line">                            transform=transform)</span><br><span class="line">loader_test = DataLoader(cifar10_test, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.</span></span><br><span class="line"><span class="keyword">for</span> t, (x, y) <span class="keyword">in</span> enumerate(loader_train):</span><br><span class="line"><span class="comment"># 2. </span></span><br><span class="line"><span class="keyword">for</span> x, y <span class="keyword">in</span> loader:</span><br></pre></td></tr></table></figure><ul><li><p>torchvision是独立于pytorch的关于图像操作的一些方便工具库。</p><p>torchvision主要包括一下几个包：</p><ul><li><a href="https://pypi.org/project/torchvision/0.1.8/#datasets" target="_blank" rel="noopener">vision.datasets</a> : 几个常用视觉数据集，可以下载和加载</li><li><a href="https://pypi.org/project/torchvision/0.1.8/#models" target="_blank" rel="noopener">vision.models</a> : 流行的模型，例如 AlexNet, VGG, and ResNet 以及 与训练好的参数。</li><li><a href="https://pypi.org/project/torchvision/0.1.8/#transforms" target="_blank" rel="noopener">vision.transforms</a> : 常用的图像操作，例如：随机切割，旋转等。</li><li><a href="https://pypi.org/project/torchvision/0.1.8/#utils" target="_blank" rel="noopener">vision.utils</a> : 用于把形似 (3 x H x W) 的张量保存到硬盘中，给一个mini-batch的图像可以产生一个图像格网。</li></ul></li><li><p>首先<code>torchvision.transform</code>：图像预处理转化，这里采取的主要就是均值归一化之后将数据转为torch.tensor</p></li><li><p><code>torchvision.dataset</code>：常用的就上述的四个参数：<code>root</code>表示数据集的存储位置，<code>train</code>用来区分训练集和测试集，<code>download</code>和<code>transorm</code>如语意所示</p></li><li><p><code>Dataloader</code>：    </p><ul><li>dataset (Dataset) – dataset from which to load the data.</li><li>batch_size (int, optional) – how many samples per batch to load (default: 1).</li><li>shuffle (bool, optional) – set to True to have the data reshuffled at every epoch (default: False).</li><li>sampler (Sampler, optional) – defines the strategy to draw samples from the dataset. If specified, shuffle must be False.</li></ul></li><li><p>两种调用loader的方式，和_iter__()很类似</p></li></ul><h1 id="Device"><a href="#Device" class="headerlink" title="Device"></a>Device</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">'cuda'</span>)</span><br><span class="line">device = torch.device(<span class="string">'cpu'</span>)</span><br></pre></td></tr></table></figure><h1 id="torch-Tensor"><a href="#torch-Tensor" class="headerlink" title="torch.Tensor"></a>torch.Tensor</h1><p>torch.Tensor几乎是综合了tensorflow中所有的constant，variable，tensor等等变量，其中有几个重要的attribute：</p><ul><li><strong>data</strong>(array_like)– The returned Tensor copies <code>data</code>.</li><li><strong>dtype</strong> (<a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype" target="_blank" rel="noopener"><code>torch.dtype</code></a>, optional) – the desired type of returned tensor. Default: if None, same <a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype" target="_blank" rel="noopener"><code>torch.dtype</code></a> as this tensor.</li><li><strong>device</strong> (<a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.device" target="_blank" rel="noopener"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if None, same <a href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.device" target="_blank" rel="noopener"><code>torch.device</code></a> as this tensor.</li><li><p><strong>requires_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a><em>,</em> <em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([[<span class="number">1.</span>, <span class="number">-1.</span>], [<span class="number">1.</span>, <span class="number">1.</span>]], requires_grad=<span class="keyword">True</span>)</span><br><span class="line">out = x.pow(<span class="number">2</span>).sum()</span><br><span class="line">out.backward()</span><br><span class="line">x.grad</span><br><span class="line">&gt;&gt;&gt;tensor([[ <span class="number">2.0000</span>, <span class="number">-2.0000</span>],</span><br><span class="line">    [ <span class="number">2.0000</span>,  <span class="number">2.0000</span>]])</span><br></pre></td></tr></table></figure></li><li><p>tensor可以通过item()（标量）和numpy()（数组）来访问数据，每个variable有两个重要的属性：data()（返回tensor）和grad()（返回梯度，numpy array）。</p></li><li><p>GPU的两种使用方式</p><ul><li>使用可以在使用的时候将tensor移动到指定的device</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> x, y <span class="keyword">in</span> loader:</span><br><span class="line">  x = x.to(device=device, dtype=dtype)  <span class="comment"># move to device, e.g. GPU</span></span><br><span class="line"> y = y.to(device=device, dtype=torch.int64)</span><br><span class="line">    scores = model_fn(x, params)</span><br></pre></td></tr></table></figure><ul><li>设定dtype</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dtype = torch.cuda.FloatTensor</span><br><span class="line">x = x.type(dtype)</span><br></pre></td></tr></table></figure></li><li><p>.view()和.reshape()类似</p></li></ul><h1 id="no-grad"><a href="#no-grad" class="headerlink" title="no.grad()"></a>no.grad()</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">   <span class="keyword">for</span> w <span class="keyword">in</span> params:</span><br><span class="line">     w -= learning_rate * w.grad</span><br><span class="line">        <span class="comment"># Manually zero the gradients after running the backward pass</span></span><br><span class="line">        w.grad.zero_()</span><br></pre></td></tr></table></figure><p>在不需要计算梯度的情况下关闭梯度，并且每次迭代都清空参数的梯度</p><h1 id="torch-nn"><a href="#torch-nn" class="headerlink" title="torch.nn"></a>torch.nn</h1><ul><li>torch.nn和torch.nn.functional，就是相同的操作，一个是class API一个是函数式API</li><li><code>torch.nn.init</code>：提供的初始化真是骚爆了。重点是每一次nn Module的初始化都要调用nn.init的初始化包</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TwoLayerFC</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, num_classes)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_size, hidden_size)</span><br><span class="line">        nn.init.kaiming_normal_(self.fc1.weight)</span><br><span class="line">        self.fc2 = nn.Linear(hidden_size, num_classes)</span><br><span class="line">        nn.init.kaiming_normal_(self.fc2.weight)</span><br><span class="line">    <span class="comment"># 调用函数位forward</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = flatten(x)</span><br><span class="line">        scores = self.fc2(F.relu(self.fc1(x)))</span><br><span class="line">        <span class="keyword">return</span> scores</span><br></pre></td></tr></table></figure><ul><li>训练过程：<strong>每一次都要手动清空梯度，否则梯度会自动累计</strong>。而且在开始之前还可以设定model的training/testing状态（应该是提供给dropout的），当然必须是<code>nn.Module</code>的继承类才可以。</li><li>.detach()可以强制使得这个点为叶节点，从而截断梯度往下传递 </li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> t, (x, y) <span class="keyword">in</span> enumerate(loader_train):</span><br><span class="line">    model.train()  <span class="comment"># put model to training mode</span></span><br><span class="line">    <span class="comment"># model.eval() put model to evaluation mode</span></span><br><span class="line">    x = x.to(device=device, dtype=dtype)  <span class="comment"># move to device, e.g. GPU</span></span><br><span class="line">    y = y.to(device=device, dtype=torch.long)</span><br><span class="line"></span><br><span class="line">    scores = model(x)</span><br><span class="line">    scores = scores.reshape((<span class="number">-1</span>, <span class="number">10</span>))</span><br><span class="line">    loss = F.cross_entropy(scores, y)</span><br><span class="line"></span><br><span class="line">   optimizer.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure><ul><li><code>nn.Sequential</code>和tf.keras.Sequential一模一样</li><li>需要移动到device的有：model、x、y</li></ul><h1 id="torch-save"><a href="#torch-save" class="headerlink" title="torch.save"></a>torch.save</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_checkpoint</span><span class="params">(checkpoint_path, model, optimizer)</span>:</span></span><br><span class="line">    state = &#123;<span class="string">'state_dict'</span>: model.state_dict(),</span><br><span class="line">             <span class="string">'optimizer'</span> : optimizer.state_dict()&#125;</span><br><span class="line">    torch.save(state, checkpoint_path)</span><br><span class="line">    print(<span class="string">'model saved to %s'</span> % checkpoint_path)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_checkpoint</span><span class="params">(checkpoint_path, model, optimizer)</span>:</span></span><br><span class="line">    state = torch.load(checkpoint_path)</span><br><span class="line">    model.load_state_dict(state[<span class="string">'state_dict'</span>])</span><br><span class="line">    optimizer.load_state_dict(state[<span class="string">'optimizer'</span>])</span><br><span class="line">    print(<span class="string">'model loaded from %s'</span> % checkpoint_path)</span><br></pre></td></tr></table></figure><ul><li>model.state_dict()</li><li>optimizer.state_dict()</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span clas
      
    
    </summary>
    
      <category term="Computer Science" scheme="http://github.com/categories/Computer-Science/"/>
    
      <category term="Deep Learning" scheme="http://github.com/categories/Computer-Science/Deep-Learning/"/>
    
    
      <category term="Pytorch" scheme="http://github.com/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow-gpu的配置</title>
    <link href="http://github.com/CUDA.html"/>
    <id>http://github.com/CUDA.html</id>
    <published>2019-01-01T02:35:57.000Z</published>
    <updated>2019-06-27T10:06:49.077Z</updated>
    
    <content type="html"><![CDATA[<p>最近在复现real time object detection方法YOLO和SSD的时候第一次真正意识到了需要使用GPU计算的紧迫性，虽然面临的系统盘SSD容量吃紧的问题，还是下定决心配置了一下Tensorflow-GPU版本，接下来对配置过程简单介绍。最终安装结束占用系统盘5GB左右的容量，其实还可以啦~</p><h1 id="CUDA"><a href="#CUDA" class="headerlink" title="CUDA"></a>CUDA</h1><p>CUDA是英伟达支持GPU操作而开发的一种语言，安装CUDA主要是将tensorflow代码转为CUDA代码从而实现GPU加速。首先需要到tensorflow官网查看tensorflow版本和CUDA和CUDNN的对应版本。</p><p><img src="/images/CUDA/1.png" alt="img"></p><p>以当时最新版的tensorflow 1.12为例，首先前往CUDA官网查看你的GPU是否支持CUDA加速，之后前往<a href="https://developer.nvidia.com/cuda-90-download-archive?target_os=Windows&amp;target_arch=x86_64&amp;target_version=10&amp;target_type=exelocal" target="_blank" rel="noopener">CUDA官网</a>下载对应版本的CUDA和CUDNN。值得注意的是CUDA不仅仅要下载主安装包，其余所有的补丁程序也都要下载。安装过程较长，且会出现屏幕闪烁，均为正常情况。CUDA安装结束之后会自动将安装地址存入<strong>系统变量</strong>。</p><h1 id="CUDNN"><a href="#CUDNN" class="headerlink" title="CUDNN"></a>CUDNN</h1><p>CUDNN作为CUDA的补充，文件较小下载速度较快。下载完成解压后，只要将“/bin”目录下的dll文件放入系统变量内即可，这里采取的操作是将该文件放到CUDA根目录的bin文件中。</p><h1 id="安装tensorflow-GPU"><a href="#安装tensorflow-GPU" class="headerlink" title="安装tensorflow-GPU"></a>安装tensorflow-GPU</h1><p>这里需要注意的是必须下载tensorflow-gpu的whl文件，普通的tensorflow.whl对应的是CPU版本。下载后可以通过输出tensorflow版本进行实验。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">tf.__version__</span><br></pre></td></tr></table></figure><p>同时过程中可见的是输出了你计算机上的GPU信息，包括可用显存等等。这样tensorflow GPU版本就安装好了。</p><h1 id="详细教程请参看视频"><a href="#详细教程请参看视频" class="headerlink" title="详细教程请参看视频"></a>详细教程请参看视频</h1><iframe width="560" height="315" src="https://www.youtube.com/embed/uIm3DMprk7M" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近在复现real time object detection方法YOLO和SSD的时候第一次真正意识到了需要使用GPU计算的紧迫性，虽然面临的系统盘SSD容量吃紧的问题，还是下定决心配置了一下Tensorflow-GPU版本，接下来对配置过程简单介绍。最终安装结束占用系统
      
    
    </summary>
    
      <category term="Computer Science" scheme="http://github.com/categories/Computer-Science/"/>
    
      <category term="Technique" scheme="http://github.com/categories/Computer-Science/Technique/"/>
    
    
      <category term="Tensorflow-GPU" scheme="http://github.com/tags/Tensorflow-GPU/"/>
    
      <category term="CUDA" scheme="http://github.com/tags/CUDA/"/>
    
  </entry>
  
  <entry>
    <title>SSD</title>
    <link href="http://github.com/SSD.html"/>
    <id>http://github.com/SSD.html</id>
    <published>2019-01-01T02:31:12.000Z</published>
    <updated>2019-01-04T23:29:42.153Z</updated>
    
    <content type="html"><![CDATA[<h1 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h1><p>SSD主要就是针对YOLO对于小物体的定位不准的问题上出发，因为YOLO只是用使用了最后一个conv层的特征，而高层的卷积层有很大的感受野，而且已经丢失了很多小的图像细节，很可能丢失了很多小object的信息，所以SSD的一个重要的创新点就是：<strong>使用了多级的feature map</strong>。</p><h3 id="设计理念"><a href="#设计理念" class="headerlink" title="设计理念"></a>设计理念</h3><p>网络基于VGG16，删除了FC层改为全卷积网络，使用从conv4_3开始的从低到高feature map信息进行判断。针对每一层m*n的feature map的每一个cell，我们使用k个不同size且不同分辨率的default box（后被映射回image），每一个default box都会作为一个archor（即待选的region proposal）计算得到c个分类置信度和4个location的修正。每一个default box的位置是固定的，我们做的就是基于default box的位置去预测bounding box的位置<strong>（直接的预测值是将default box映射回image后的位置修正）</strong>。</p><p>测试时很简单，SSD输出固定数量的bounding box及其对应的C个分类置信度，之后再做NMS即可。</p><p><img src="/images/SSD/1.png" alt="img"></p><h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><ol><li>Matching：首先最重要的是将default box于ground truth匹配。（1）将ground truth和与之IOU最高的default box匹配；（2）将ground truth和每一个与之IOU&gt;0.5的default box匹配。同样的一个ground truth可以匹配度个default box，但是一个default box只能匹配一个ground truth。</li><li>Loss Function：与几个state-of-art的算法相同，是location loss和classification loss的加权和。</li><li>手动选择每一层的default box的size和长宽比：这一步实际上等于是手动完成了RPN</li><li>Hard negative mining：与R-CNN相同使用了这个trick来加快训练。选择background类置信度较低的（较难区别为背景类）default box加入训练。</li><li>Data augmentation：至关重要的一步，我认为应该是通过片段的截取和翻转，让网络可以直接关注图片的那些小的部分的信息，实际上是手动的将细节信息放大让深层的卷积层仍然保留这部分的信息，从而更好的识别小物体。</li></ol><h2 id="tensorflow源码分析"><a href="#tensorflow源码分析" class="headerlink" title="tensorflow源码分析"></a>tensorflow源码分析</h2><p>主要通过分析test time的代码弄清实现细节。我们以SSD300的网络结构为例，SSD300基于VGG16，将FC层换成卷积层并且新增了多层卷积用来做多级预测。高层特征图检测小物体，底层特征图检测大物体。</p><ol><li>首先我们先通过网络的几个默认参数分析几个implement的细节：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">self.ssd_params = SSDParams(img_shape=(<span class="number">300</span>, <span class="number">300</span>),</span><br><span class="line">                            num_classes=<span class="number">21</span>,</span><br><span class="line">                            no_annotation_label=<span class="number">21</span>,</span><br><span class="line">                            feat_layers=[<span class="string">"block4"</span>, <span class="string">"block7"</span>, <span class="string">"block8"</span>, <span class="string">"block9"</span>, <span class="string">"block10"</span>, <span class="string">"block11"</span>],</span><br><span class="line">                            feat_shapes=[(<span class="number">38</span>, <span class="number">38</span>), (<span class="number">19</span>, <span class="number">19</span>), (<span class="number">10</span>, <span class="number">10</span>), (<span class="number">5</span>, <span class="number">5</span>), (<span class="number">3</span>, <span class="number">3</span>), (<span class="number">1</span>, <span class="number">1</span>)],</span><br><span class="line">                            anchor_size_bounds=[<span class="number">0.15</span>, <span class="number">0.90</span>],  <span class="comment"># diff from the original paper</span></span><br><span class="line">                            anchor_sizes=[(<span class="number">21.</span>, <span class="number">45.</span>),</span><br><span class="line">                                          (<span class="number">45.</span>, <span class="number">99.</span>),</span><br><span class="line">                                          (<span class="number">99.</span>, <span class="number">153.</span>),</span><br><span class="line">                                          (<span class="number">153.</span>, <span class="number">207.</span>),</span><br><span class="line">                                          (<span class="number">207.</span>, <span class="number">261.</span>),</span><br><span class="line">                                          (<span class="number">261.</span>, <span class="number">315.</span>)],</span><br><span class="line">                            anchor_ratios=[[<span class="number">2</span>, <span class="number">.5</span>],</span><br><span class="line">                                           [<span class="number">2</span>, <span class="number">.5</span>, <span class="number">3</span>, <span class="number">1.</span> / <span class="number">3</span>],</span><br><span class="line">                                           [<span class="number">2</span>, <span class="number">.5</span>, <span class="number">3</span>, <span class="number">1.</span> / <span class="number">3</span>],</span><br><span class="line">                                           [<span class="number">2</span>, <span class="number">.5</span>, <span class="number">3</span>, <span class="number">1.</span> / <span class="number">3</span>],</span><br><span class="line">                                           [<span class="number">2</span>, <span class="number">.5</span>],</span><br><span class="line">                                           [<span class="number">2</span>, <span class="number">.5</span>]],</span><br><span class="line">                            anchor_steps=[<span class="number">8</span>, <span class="number">16</span>, <span class="number">32</span>, <span class="number">64</span>, <span class="number">100</span>, <span class="number">300</span>],</span><br><span class="line">                            anchor_offset=<span class="number">0.5</span>,</span><br><span class="line">                            normalizations=[<span class="number">20</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>],</span><br><span class="line">                            prior_scaling=[<span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.2</span>]</span><br><span class="line">                           )</span><br></pre></td></tr></table></figure><ul><li><p>feat_shapes代表的是用来检测的几个特征图的尺度（W&amp;H）。以此作为条件我们计算下面的参数</p></li><li><p>anchor_size_bounds与原文的范围有不同，anchor_sizes中的元素指的是该层<strong>先验框的大小（即S乘300之后的结果）</strong>。论文中给出的s指的是先验框大小和 <strong>原图片</strong>大小的比值。除了第一层block4特别设置以外，其余层遵循以下规则：（其中k为层数，计算值为百分比，乘300之前要除以100，<a href="https://blog.csdn.net/qq_36735489/article/details/83653816" target="_blank" rel="noopener">深度好文</a>）</p><p><img src="/images/SSD/2.png" alt="img"></p></li><li><p>每一层有两个先验框的大小：<strong>min_size和max_size</strong>，以这两个值计算该层特征图的多个先验框：一个是分别以min_size和sqrt(max_size * min_size)为边长的正方形，然后根据该层的anchor_ratios得到和min_size的正方形面积相同的矩形先验框。(<a href="https://blog.csdn.net/qq_36735489/article/details/83654443" target="_blank" rel="noopener">深度好文</a>)</p><p><img src="/images/SSD/3.png" alt="img"></p></li><li><p>anchor_step：最神奇之处，在源码中SSD将feature map和image相应点的映射实际上不是用SPP的逆映射，而是（以block4为例）将38个特征点平均分布在image上，即8=300/38向上取整。另外offset指的是在image上的取点的修正量，以防将image上的对应位置选在了corner上。</p></li></ul><ol start="2"><li>然后我们来看一下Multi-Box layer</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ssd_multibox_layer</span><span class="params">(x, num_classes, sizes, ratios, normalization=<span class="number">-1</span>, scope=<span class="string">"multibox"</span>)</span>:</span></span><br><span class="line">    pre_shape = x.get_shape().as_list()[<span class="number">1</span>:<span class="number">-1</span>] <span class="comment"># [W, H]</span></span><br><span class="line">    pre_shape = [<span class="number">-1</span>] + pre_shape <span class="comment"># [-1, W, H]</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">        <span class="comment"># l2 norm</span></span><br><span class="line">        <span class="keyword">if</span> normalization &gt; <span class="number">0</span>:</span><br><span class="line">            x = l2norm(x, normalization)</span><br><span class="line">            print(x)</span><br><span class="line">        <span class="comment"># numbers of anchors</span></span><br><span class="line">        n_anchors = len(sizes) + len(ratios)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># location predictions</span></span><br><span class="line">        loc_pred = conv2d(x, n_anchors*<span class="number">4</span>, <span class="number">3</span>, activation=<span class="keyword">None</span>, scope=<span class="string">"conv_loc"</span>)</span><br><span class="line">        loc_pred = tf.reshape(loc_pred, pre_shape + [n_anchors, <span class="number">4</span>])</span><br><span class="line">        <span class="comment"># class prediction</span></span><br><span class="line">        cls_pred = conv2d(x, n_anchors*num_classes, <span class="number">3</span>, activation=<span class="keyword">None</span>, scope=<span class="string">"conv_cls"</span>)</span><br><span class="line">        cls_pred = tf.reshape(cls_pred, pre_shape + [n_anchors, num_classes])</span><br><span class="line">        <span class="keyword">return</span> cls_pred, loc_pred</span><br></pre></td></tr></table></figure><ul><li>首先conv4_3在进行检测过程前要做一个L2_normalization</li><li>n_archors可以验证上面对于先验框尺寸的分析</li><li>location和classification都是简单的用一个3×3的conv层进行检测的</li><li>之后进行bounding box的筛选，</li></ul><ol start="3"><li>再来看一下archor默认先验框的生成过程</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ssd_anchor_one_layer</span><span class="params">(img_shape,</span></span></span><br><span class="line"><span class="function"><span class="params">                         feat_shape,</span></span></span><br><span class="line"><span class="function"><span class="params">                         sizes,</span></span></span><br><span class="line"><span class="function"><span class="params">                         ratios,</span></span></span><br><span class="line"><span class="function"><span class="params">                         step,</span></span></span><br><span class="line"><span class="function"><span class="params">                         offset=<span class="number">0.5</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                         dtype=np.float32)</span>:</span></span><br><span class="line">    y, x = np.mgrid[<span class="number">0</span>:feat_shape[<span class="number">0</span>], <span class="number">0</span>:feat_shape[<span class="number">1</span>]]</span><br><span class="line">    y = (y.astype(dtype) + offset) * step / img_shape[<span class="number">0</span>]</span><br><span class="line">    x = (x.astype(dtype) + offset) * step / img_shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Expand dims to support easy broadcasting.</span></span><br><span class="line">    y = np.expand_dims(y, axis=<span class="number">-1</span>)  <span class="comment"># [size, size, 1]</span></span><br><span class="line">    x = np.expand_dims(x, axis=<span class="number">-1</span>)  <span class="comment"># [size, size, 1]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute relative height and width.</span></span><br><span class="line">    <span class="comment"># Tries to follow the original implementation of SSD for the order.</span></span><br><span class="line">    num_anchors = len(sizes) + len(ratios)</span><br><span class="line">    h = np.zeros((num_anchors, ), dtype=dtype)  <span class="comment"># [n_anchors]</span></span><br><span class="line">    w = np.zeros((num_anchors, ), dtype=dtype)  <span class="comment"># [n_anchors]</span></span><br><span class="line">    <span class="comment"># Add first anchor boxes with ratio=1.</span></span><br><span class="line">    h[<span class="number">0</span>] = sizes[<span class="number">0</span>] / img_shape[<span class="number">0</span>]</span><br><span class="line">    w[<span class="number">0</span>] = sizes[<span class="number">0</span>] / img_shape[<span class="number">1</span>]</span><br><span class="line">    di = <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> len(sizes) &gt; <span class="number">1</span>:</span><br><span class="line">        h[<span class="number">1</span>] = math.sqrt(sizes[<span class="number">0</span>] * sizes[<span class="number">1</span>]) / img_shape[<span class="number">0</span>]</span><br><span class="line">        w[<span class="number">1</span>] = math.sqrt(sizes[<span class="number">0</span>] * sizes[<span class="number">1</span>]) / img_shape[<span class="number">1</span>]</span><br><span class="line">        di += <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i, r <span class="keyword">in</span> enumerate(ratios):</span><br><span class="line">        h[i+di] = sizes[<span class="number">0</span>] / img_shape[<span class="number">0</span>] / math.sqrt(r)</span><br><span class="line">        w[i+di] = sizes[<span class="number">0</span>] / img_shape[<span class="number">1</span>] * math.sqrt(r)</span><br><span class="line">    <span class="keyword">return</span> y, x, h, w</span><br></pre></td></tr></table></figure><ul><li>每一个bbo记录中心点坐标和长宽。中心点坐标是相对于Image size的(0, 1)的值，长宽的生成验证了上述的不同尺度的先验框的解释。</li></ul><ol start="4"><li>再看一下bbox的解码过程，分析直接预测值究竟是什么？</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_bboxes_decode_layer</span><span class="params">(self, feat_locations, anchor_bboxes, prior_scaling)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Decode the feat location of one layer</span></span><br><span class="line"><span class="string">    params:</span></span><br><span class="line"><span class="string">     feat_locations: 5D Tensor, [batch_size, size, size, n_anchors, 4]</span></span><br><span class="line"><span class="string">     anchor_bboxes: list of Tensors(y, x, w, h)</span></span><br><span class="line"><span class="string">                    shape: [size,size,1], [size, size,1], [n_anchors], [n_anchors]</span></span><br><span class="line"><span class="string">     prior_scaling: list of 4 floats</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    yref, xref, href, wref = anchor_bboxes</span><br><span class="line">    print(yref)</span><br><span class="line">    <span class="comment"># Compute center, height and width</span></span><br><span class="line">    cx = feat_locations[:, :, :, :, <span class="number">0</span>] * wref * prior_scaling[<span class="number">0</span>] + xref </span><br><span class="line">    <span class="comment"># 中心点相对于中心点的位移</span></span><br><span class="line">    cy = feat_locations[:, :, :, :, <span class="number">1</span>] * href * prior_scaling[<span class="number">1</span>] + yref</span><br><span class="line">    w = wref * tf.exp(feat_locations[:, :, :, :, <span class="number">2</span>] * prior_scaling[<span class="number">2</span>]) <span class="comment"># 长宽拉伸</span></span><br><span class="line">    h = href * tf.exp(feat_locations[:, :, :, :, <span class="number">3</span>] * prior_scaling[<span class="number">3</span>])</span><br><span class="line">    <span class="comment"># compute boxes coordinates (ymin, xmin, ymax,,xmax), 坐标转换，为了更好的画框</span></span><br><span class="line">    bboxes = tf.stack([cy - h / <span class="number">2.</span>, cx - w / <span class="number">2.</span>,</span><br><span class="line">                       cy + h / <span class="number">2.</span>, cx + w / <span class="number">2.</span>], axis=<span class="number">-1</span>)</span><br><span class="line">    <span class="comment"># shape [batch_size, size, size, n_anchors, 4]</span></span><br><span class="line">    <span class="keyword">return</span> bboxes</span><br></pre></td></tr></table></figure><ul><li>可以看到直接预测值仍然是训练过程中所定义的修正值。</li></ul><ol start="5"><li>对于所有预测先验框的筛选过程</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_bboxes</span><span class="params">(rclasses, rscores, rbboxes, rbbox_img = <span class="params">(<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>)</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                   top_k=<span class="number">400</span>, nms_threshold=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Process the bboxes including sort and nms"""</span></span><br><span class="line">    rbboxes = bboxes_clip(rbbox_img, rbboxes)</span><br><span class="line">    rclasses, rscores, rbboxes = bboxes_sort(rclasses, rscores, rbboxes, top_k)</span><br><span class="line">    rclasses, rscores, rbboxes = bboxes_nms(rclasses, rscores, rbboxes, nms_threshold)</span><br><span class="line">    rbboxes = bboxes_resize(rbbox_img, rbboxes)</span><br><span class="line">    <span class="keyword">return</span> rclasses, rscores, rbboxes</span><br></pre></td></tr></table></figure><ul><li>在这个函数之前实际上先做了一个class score&gt;thresold的阈值判断，接下来做clip处理超出图片范围的bbox（强制拉回图片而不是删除，删除是针对与训练的时候的），然后是将<strong>所有类别</strong>的bbox class score在一起排序选择前400个之后做NMS非极大值抑制之后输出。</li><li>得到的rbboxes是一个[-1, 4]的numpy二维矩阵，4个坐标分别是左上角的横纵坐标和右下角的横纵坐标。（从结果可以看到原点在图像的左上方）</li></ul><ol start="6"><li>最后看一下我根据YOLO的show_result函数改写的使用<strong>open_cv</strong>展示结果的函数：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_results</span><span class="params">(image, classes, scores, bboxes)</span>:</span></span><br><span class="line">    img_cp = image.copy()</span><br><span class="line">    img_cp = cv2.cvtColor(img_cp, cv2.COLOR_BGR2RGB)</span><br><span class="line">    height =  img_cp.shape[<span class="number">0</span>]</span><br><span class="line">    width = img_cp.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(classes)):</span><br><span class="line">        ymin = int(bboxes[i, <span class="number">0</span>] * height)</span><br><span class="line">        xmin = int(bboxes[i, <span class="number">1</span>] * width)</span><br><span class="line">        ymax = int(bboxes[i, <span class="number">2</span>] * height)</span><br><span class="line">        xmax = int(bboxes[i, <span class="number">3</span>] * width)</span><br><span class="line">    </span><br><span class="line">        cv2.rectangle(img_cp, (xmin, ymin), (xmax, ymax), (<span class="number">0</span>, <span class="number">255</span>, <span class="number">0</span>), <span class="number">2</span>)</span><br><span class="line">        cv2.rectangle(img_cp, (xmin, ymin - <span class="number">20</span>), (xmax,ymin), (<span class="number">125</span>, <span class="number">125</span>, <span class="number">125</span>), <span class="number">-1</span>)</span><br><span class="line">        cv2.putText(img_cp, CLASSES[classes[i] - <span class="number">1</span>] + <span class="string">' : %.2f'</span> % scores[i], (xmin - <span class="number">5</span>, ymin - <span class="number">7</span>), </span><br><span class="line">        cv2.FONT_HERSHEY_SIMPLEX, <span class="number">0.5</span>, (<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>), <span class="number">1</span>)</span><br><span class="line">        cv2.imshow(<span class="string">'SSD'</span>, img_cp)</span><br><span class="line">        cv2.waitKey(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><ul><li>使用open_cv读取展示图像都需要转换RGB的顺序</li><li>cv2.rectangle(img, (x, y), (x’, y’), (255, 255, 255), 2)。参数解释：原图、左上角坐标，右下角坐标，框是哟个的颜色，框的粗细。注意坐标一定要用整数</li><li>cv2.putText()，参数依次是：照片/添加的文字/左上角坐标/字体/字体大小/颜色/字体粗细</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://blog.csdn.net/xiaohu2022/article/details/79833786" target="_blank" rel="noopener">CSDN解析1</a></li><li><a href="https://blog.csdn.net/WZZ18191171661/article/details/79444217" target="_blank" rel="noopener">CSDN解析2</a></li><li><a href="http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf" target="_blank" rel="noopener">ECCV Slide</a></li><li><a href="https://docs.google.com/presentation/d/1rtfeV_VmdGdZD5ObVVpPDPIODSDxKnFSU0bsN_rgZXc/pub?start=false&amp;loop=false&amp;delayms=3000&amp;slide=id.g179f601b72_0_51" target="_blank" rel="noopener">俄罗斯非常优秀的细节解析</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;SSD&quot;&gt;&lt;a href=&quot;#SSD&quot; class=&quot;headerlink&quot; title=&quot;SSD&quot;&gt;&lt;/a&gt;SSD&lt;/h1&gt;&lt;p&gt;SSD主要就是针对YOLO对于小物体的定位不准的问题上出发，因为YOLO只是用使用了最后一个conv层的特征，而高层的卷积层有很大的
      
    
    </summary>
    
      <category term="Computer Science" scheme="http://github.com/categories/Computer-Science/"/>
    
      <category term="Computer Vision" scheme="http://github.com/categories/Computer-Science/Computer-Vision/"/>
    
    
      <category term="Object Detection" scheme="http://github.com/tags/Object-Detection/"/>
    
      <category term="Paper Reading" scheme="http://github.com/tags/Paper-Reading/"/>
    
  </entry>
  
  <entry>
    <title>CK的二手交易市场</title>
    <link href="http://github.com/sale.html"/>
    <id>http://github.com/sale.html</id>
    <published>2018-12-20T12:50:50.000Z</published>
    <updated>2019-01-05T00:03:36.877Z</updated>
    
    <content type="html"><![CDATA[<h1 id="曼城"><a href="#曼城" class="headerlink" title="曼城"></a>曼城</h1><p>本人1月27日回国故出二手。不包邮，请到<a href="http://www.accommodation.manchester.ac.uk/search/details/?property=53" target="_blank" rel="noopener">Weston Hall</a>自取！</p><table><thead><tr><th style="text-align:center">Image</th><th>Price（只收RMB哈哈）</th><th style="text-align:center">Description</th></tr></thead><tbody><tr><td style="text-align:center"><img src="/images/sale/4.jpg" width="500" height="300"></td><td>￥25</td><td style="text-align:center">Tesco大炒锅 + 锅勺铲套件 + 各种调味料</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;曼城&quot;&gt;&lt;a href=&quot;#曼城&quot; class=&quot;headerlink&quot; title=&quot;曼城&quot;&gt;&lt;/a&gt;曼城&lt;/h1&gt;&lt;p&gt;本人1月27日回国故出二手。不包邮，请到&lt;a href=&quot;http://www.accommodation.manchester.ac.uk
      
    
    </summary>
    
    
      <category term="二手" scheme="http://github.com/tags/%E4%BA%8C%E6%89%8B/"/>
    
  </entry>
  
  <entry>
    <title>R-CNN</title>
    <link href="http://github.com/R-CNN.html"/>
    <id>http://github.com/R-CNN.html</id>
    <published>2018-12-20T02:33:00.000Z</published>
    <updated>2019-01-01T11:25:02.277Z</updated>
    
    <content type="html"><![CDATA[<h1 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h1><p>R-CNN由UCB的视觉实验室提出，将传统的视觉方法selective search和CNN结合到一起。实际上我感觉应该是受到了当时Alexnet的启发，因为当时已经有使用selective search得到候选区域再进行分类的做法，只不过没有用到CNN而已。主要思路就是先在Input Image上做selective search得到2000个region proposal，再<strong>把每个proposal wrap为227 <em> 227 </em> 3 的形式</strong>（需要resize）输入CNN，再输入到<strong>SVM分类器得</strong>到分类和一个<strong>Bounding Box regressor</strong>得到box的修正。</p><h2 id="Training-Process"><a href="#Training-Process" class="headerlink" title="Training Process"></a>Training Process</h2><ul><li>Region proposal：使用selective search生成2000个候选区域。</li><li>Fine Tuning：我认为Fine tuning本质上还是<strong>图像分类问题</strong>，将每一个region当作独立的Image，只是为了适应新的domain。首先使用Alexnet在image classification dataset上预训练之后，<strong>做fine tuning</strong>。为了适应新的数据集只有N = 20&lt;1000的情况，我们保持原有Alexnet的结构不变，只是重新初始化了分类器，使用一个<strong>(N + 1) classes</strong> 分类器。我们使用wrapped region peoposals，将与任意一个<strong>ground truth IOU&gt;0.5</strong>的region proposal作为正样本，剩余作为负样本。这里我认为正样本值得是有object的region，其中包括它的calss label，负样本为background样本。每一个batch包括32个正样本和96个负样本。</li><li>SVM detection Classifier：SVM用于detection。训练SVM的过程，我们只取ground truth作为正样本，取和ground truth的IOU小于0.3的为负样本。</li><li>训练过程中我们会碰到如何判断一个训练结果（即分类结果是对是错），如果一个region只是部分overlap ground truth，那么我们就计算这两个部分的IOU，如果&gt;阈值<strong>（这里为0.3，和上面的不同）</strong>就判断为正样本，否则为负样本，这也是一种hard negative mining的训练方式，可以帮助更快收敛，下面做详细介绍。</li><li>Bounding Box regression：我们也对bounding box做回归修正。我们直接对四个坐标做修正（回归器输出是修正值），但是不直接用四个坐标为输入，而是使用这个region proposal的<strong>pool5 feature vector</strong>作为函数的输入，对x，y做平移，对w，y做放缩。此外我们学习的函数都是线性函数，就只用一个权值矩阵就好了。具体公式请参考论文。注意这里的对应关系根据IOU&gt;0.6获得。这里P是selective search的结果，G~是修正结果。</li><li>R-CNN训练过程分为三个独立的步骤：fine tuning，SVM然后才是bounding box regressor。这样必须分开来训练导致了训练时间的浪费。</li></ul><p><img src="/images/R-CNN/1.png" alt="img"></p><h3 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h3><p>有监督预训练也称之为迁移学习，举例说明：若有大量标注信息的人脸年龄分类的正负样本图片，利用样本训练了CNN网络用于人脸年龄识别；现在要通过人脸进行性别识别，那么就可以去掉已经训练好的人脸年龄识别网络CNN的最后一层或几层，换成所需要的分类层，前面层的网络参数直接使用为初始化参数，修改层的网络参数随机初始化，再利用人脸性别分类的正负样本图片进行训练，得到人脸性别识别网络，这种方法就叫做有监督预训练。这种方式可以很好地解决小样本数据无法训练深层CNN网络的问题，我们都知道小样本数据训练很容易造成网络过拟合，但是在大样本训练后利用其参数初始化网络可以很好地训练小样本，这解决了小样本训练的难题。这篇文章最大的亮点就是采用了这种思想，ILSVRC样本集上用于图片分类的含标注类别的训练集有1millon之多，总共含有1000类；而PASCAL VOC 2007样本集上用于物体检测的含标注类别和位置信息的训练集只有10k，总共含有20类，直接用这部分数据训练容易造成过拟合，因此文中利用ILSVRC2012的训练集先进行有监督预训练。</p><h3 id="hard-negative-mining"><a href="#hard-negative-mining" class="headerlink" title="hard negative mining"></a>hard negative mining</h3><ol><li>hard-negative mining<br>negative相对于positive，是相对于正样本来说，不含有目标的负样本。但是，negative包括很多，有完全不包含目标的的，也有部分含有的，也就是容易/不容易被分为负样本的。其中比较容易被判定是负样本（比如全是背景）的对于训练并不能起到很好的监督作用。我们需要找一些难划分的负样本，也就是hard negative，来增强网络的判别性能。在训练好分类器之后进行分类，但是发现，分类器效果不是很好，经常会得到一些错误的正样本，这个时候这些判别错误的样本可以作为负样本继续训练网络。</li><li>具体的流程</li></ol><ul><li>在检测问题中，预定义的分类器得到的错误结果，也就是FP，记录其对应特征和分类器得到的概率。</li><li>重新训练，按概率值进行排序，再使用排序后的对应特征重新训练分类器</li><li>迭代以上过程</li></ul><h2 id="Testing-Process"><a href="#Testing-Process" class="headerlink" title="Testing Process"></a>Testing Process</h2><p>没有太多的变动，过程中由于没有ground truth不需要对2000个region proposal进行取舍（也因此成为了瓶颈）。测试的时候使用N=20个分类器，每个region proposal得到20个分类得分，然后在每一个class空间中做<strong>大于阈值</strong>检测和<strong>non-maximal suppression</strong>后输出结果。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="http://export.arxiv.org/pdf/1311.2524" target="_blank" rel="noopener">Original Paper</a></li><li><a href="https://www.cnblogs.com/zf-blog/p/6740736.html" target="_blank" rel="noopener">CSDN解析</a></li></ol><h1 id="Fast-CNN"><a href="#Fast-CNN" class="headerlink" title="Fast CNN"></a>Fast CNN</h1><p>R-CNN虽然显著的提高了mAP，但是算法耗时太长，每一个Region Proposal都要从零开始经过卷积运算。SSPNet发现了这个问题，开始使用共享卷积运算，即CNN直接输入Image获得Feature Map，在feature map的基础上进行判断，这样针对于同一张图片的ROI share了卷积运算，但是对于不同Image的ROI则不可以共享（因为back pro的时候使用的前向参数不同），另外SSPNet和R-CNN都存在不能end to end 的训练，即fine tuning、classifier和bounding box regressor必须分开来训练。Fast R-CNN针对以上问题提出改进，在稳定提高mAP的同时极大的改善了训练和测试时间。</p><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><h3 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h3><p>基于VGG16。训练过程没有太大的改动，现在Image上查找ROI，然后将Image输入到CNN当中获得feature map，通过与SSPNet相同的<strong>ROI projection</strong>找到feature map中与ROI对应的位置，然后通过ROI pooling得到fixed-sized输出，通过FC层得到ROI feature vector，通过分类器（K+1 classes）和bounding box regressor（每个class一个4元组）得到输出。注意，<strong>这里每个ROI有K个bounding box输出，对应K类</strong>。</p><h3 id="ROI-projection"><a href="#ROI-projection" class="headerlink" title="ROI projection"></a>ROI projection</h3><p>ROI projection——来自于SSPNet，其实就是找到feature map上面对应的ROI的feature。</p><h3 id="ROI-pooling-layer"><a href="#ROI-pooling-layer" class="headerlink" title="ROI pooling layer"></a>ROI pooling layer</h3><p>Spatial pyramid pooling network：SSP对于proposal的feature map使用了不同size的pooling，从而获得了不同size的fix-sized output之后连接起来一起作为输出。</p><p>ROI pooling与之相同的是，两者都是为了将no-fixed size的ROI<strong>（本文中讨论的所有ROI都基于feature map）</strong>转化为fix-sized输出。ROI pooling layer可以看作是只有一层的pyramid layer。（文中也证明了使用multi-scale并不会带来显著效果，但是会严重提升时间消耗）</p><h3 id="Process：end-to-end-training"><a href="#Process：end-to-end-training" class="headerlink" title="Process：end to end training"></a>Process：end to end training</h3><p>Fast R-CNN的一个重要特点就是可以end to end的使用back prop训练所有的参数。同样的，先去预训练好的VGG模型，去掉最后一个max pooling layer用ROI pooling层代替，1000-class sotfmax分类器用两个分支网络代替。SSP和R-CNN因为不是end-end的网络，所以不属于同一个image的ROI不能share之前的卷积运算，因此造成了训练瓶颈。</p><p>这里，我们使用<strong>Hierarchically sample</strong>的方法构造batch的方式：先sample N( = 2)张image，然后每张图片中sample R/N个ROI，较小的N保证了share。使用Multi-loss训练分类器和Regressor（预测值仍然是offset），只有正样本才有bounding box loss，其中regression loss使用smooth L1 loss。<strong>每一个class都有一个自己的regressor。</strong></p><p><img src="/images/R-CNN/2.png" alt="img"></p><h2 id="Testing"><a href="#Testing" class="headerlink" title="Testing"></a>Testing</h2><p>一次前馈即可，最后和R-CNN相同，在每个类别空间里做阈值检测和非极大值抑制之后输出结果。</p><h2 id="Reference-1"><a href="#Reference-1" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf" target="_blank" rel="noopener">paper</a></li><li><a href="https://blog.csdn.net/shenxiaolu1984/article/details/51036677" target="_blank" rel="noopener">CSDN</a></li></ol><h1 id="Faster-CNN"><a href="#Faster-CNN" class="headerlink" title="Faster CNN"></a>Faster CNN</h1><p>主要就是解决了selective search的时间瓶颈。不再使用固定的方式来生成ROI，而是使用RPN生成ROI后输入到Fast R-CNN中进行判断。注意本文与Fast R-CNN相同，所有的ROI均在feature map层面讨论，注意区分ROI，bounding box，archor这几个概念。</p><h2 id="Region-Proposal-Network-（RPN）"><a href="#Region-Proposal-Network-（RPN）" class="headerlink" title="Region Proposal Network （RPN）"></a>Region Proposal Network （RPN）</h2><p>sliding window network：取最后一个conv层（i.e.feature map），使用一个n <em> n (n=3 here)滑过feature map，生成更低纬度的feature vector，然后输出到一个二元分类器和一个regressor中<strong>（均为FC网络）</strong>。每一个sliding window会产生k个不同size and 分辨率的archor，一共会产生W </em> H <em> k个predicting（WH为feature map的维度）。一般来说是先输入到一个3 </em> 3的卷积层之后分别输入到两个1 * 1的卷积层中得到输出。  </p><h3 id="Archor"><a href="#Archor" class="headerlink" title="Archor"></a>Archor</h3><p>首先我们需要知道anchor的本质是什么，本质是SPP(spatial pyramid pooling)思想的逆向。而SPP本身是做什么的呢，就是将不同尺寸的输入resize成为相同尺寸的输出。所以SPP的逆向就是，将相同尺寸的输出，倒推得到不同尺寸的输入。<strong>也就是说通过我们当前的sliding window来推断出原始图像中对应的box，即archor是存在于原始图像的，从下文我们可以看出，由于我们逆向选择的是sliding window的中心点对应原图的感受野的中心点，所以我们可以根据不同的分辨率和size选择不同的archor。</strong></p><p>对于每个3x3的窗口，计算这个<strong>滑动窗口的中心点</strong>所对应的<strong>原始图片的中心点</strong>。然后作者假定，这个3x3窗口，是从原始图片上通过SPP池化得到的，而这个池化的区域的面积以及比例，就是一个个的anchor。换句话说，对于每个3x3窗口，作者假定它来自9种不同原始区域的池化，但是这些池化在原始图片中的中心点，都完全一样。这个中心点，就是刚才提到的，3x3窗口中心点所对应的原始图片中的中心点。如此一来，在每个窗口位置，我们都可以根据9个不同长宽比例、不同面积的anchor，逆向推导出它所对应的原始图片中的一个区域，这个区域的尺寸以及坐标，都是已知的。所以我们通过滑动窗口和anchor，成功得到了 51x39x9 个原始图片的proposal。</p><p>我觉得这是设置Archor，就像是YOLO中的grid，是为了给ground truth和predicting box设置基准的。由于Archor具有不同的尺寸，<strong>根据每一个特定的尺寸存在一个特定的bounding box regressor</strong>。</p><h3 id="RPN-Training"><a href="#RPN-Training" class="headerlink" title="RPN Training"></a>RPN Training</h3><p>首先<strong>标记Archor</strong>，这是一个二元分类问题。将两类archor标记为正样本：（1）每一个ground truth重合的IOU最大的archor标为正样本（2）和任意ground truth的IOU大于0.7的archor标为正样本，因此每个ground truth可能被分配给好几个archor box。此外，将和所有ground truth的IOU都小于0.3的标记为负样本，注意<strong>存在非正且非负的样本</strong>。下面给出Loss Function：</p><p><img src="/images/R-CNN/3.png" alt="img"></p><p>i是mini batch中 archor的索引。pi表示是Object的概率。第二部分是bounding box regression，只有正样本才有这一项。ti代表四个locational parameters，让predicting box从archor的位置回归到ground truth的位置。每一个特定尺寸的archor存在一个特定的bounding box regressor。和其他R-CNN不同的是，这里的直接输出值实际上就是bounding box在image上的坐标。和Fast R-CNN一样，先sample image再sample archor，batch中正负archor数量相同。</p><h2 id="Share-features-with-Fast-R-CNN"><a href="#Share-features-with-Fast-R-CNN" class="headerlink" title="Share features with Fast R-CNN"></a>Share features with Fast R-CNN</h2><ul><li>Alternatiining Training</li></ul><p>为了将两个网络作为整理训练，文中使用一种厉害的<strong>4-Step Alternating Training</strong>：不管是RPN还是Fast R-CNN网络，其网络结构一部分来自于pre-trained model的卷积层（下文简称model），另一部分则是他们各自特有的结构（有卷积和FC，下文简称unique）。</p><ol><li><p>用pre-trained model初始化RPN网络，然后训练RPN，在训练后，model以及RPN的unique会被更新。</p></li><li><p>用pre-trained model初始化Fast-rcnn网络，注意这个model的初始化并不是使用1中更新的model。然后使用训练过的RPN来计算proposal，再将proposal给予Fast-rcnn网络。接着训练Fast-rcnn。训练完以后，model以及Fast-rcnn的unique都会被更新。<br>说明：第一和第二步，用同样的model初始化RPN网络和Fast-rcnn网络，然后各自独立地进行训练，所以训练后，各自对model的更新一定是不一样的，因此就意味着model是不共享的。</p></li><li><p>使用第二步训练完成的model来初始化RPN网络，第二次训练RPN网络。但是这次要把model锁定，训练过程中，model始终保持不变，而RPN的unique会被改变。<br>说明：因为这一次的训练过程中，model始终保持和上一步Fast-rcnn中model一致，所以就称之为着共享。</p></li><li><p>仍然保持第三步的model不变，初始化Fast-rcnn，第二次训练Fast-rcnn网络。其实就是对其unique进行finetune，训练完毕，得到一个文中所说的unified network。</p></li></ol><ul><li>End to end training</li></ul><p>这种方式就是直接的端到端的训练方式，由RPN生成region proposal和Loss_1，然后将RPN得到的proposal输入到Fast R-CNN中得到detection Loss_2，加和两个loss做Multi-task training。</p><h2 id="Details"><a href="#Details" class="headerlink" title="Details"></a>Details</h2><ul><li>训练的时候我们不采用任何超过边界的archor，但是测试的时候我们使用clip的方式，即填充。</li><li>RPN得到的box实际上是image级别的，但是我们可以根据数学关系得到feature map级别的proposal，再通过ROI pooling得到<strong>固定尺寸</strong>的proposal输入到Fast R-CNN中</li><li>在test time通过RPN会生成很多region proposal，先通过NMS删除重合度高的proposal，再根据object score选择score值最高的N个（这里最好的值为300）proposal。可以发现数量大大减少，同时得到了mAP的提升。</li></ul><h2 id="Referece"><a href="#Referece" class="headerlink" title="Referece"></a>Referece</h2><ol><li><a href="http://de.arxiv.org/pdf/1506.01497" target="_blank" rel="noopener">paper</a></li><li><a href="https://zhuanlan.zhihu.com/p/24916624" target="_blank" rel="noopener">解析</a></li><li><a href="https://blog.csdn.net/zziahgf/article/details/79311275" target="_blank" rel="noopener">CSDN</a></li><li><a href="https://zhuanlan.zhihu.com/p/24780433" target="_blank" rel="noopener">如何将ROI映射回原图</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;R-CNN&quot;&gt;&lt;a href=&quot;#R-CNN&quot; class=&quot;headerlink&quot; title=&quot;R-CNN&quot;&gt;&lt;/a&gt;R-CNN&lt;/h1&gt;&lt;p&gt;R-CNN由UCB的视觉实验室提出，将传统的视觉方法selective search和CNN结合到一起。实际上我感觉
      
    
    </summary>
    
      <category term="Computer Science" scheme="http://github.com/categories/Computer-Science/"/>
    
      <category term="Computer Vision" scheme="http://github.com/categories/Computer-Science/Computer-Vision/"/>
    
    
      <category term="Object Detection" scheme="http://github.com/tags/Object-Detection/"/>
    
      <category term="Paper Reading" scheme="http://github.com/tags/Paper-Reading/"/>
    
  </entry>
  
  <entry>
    <title>YOLO (You Only Look Once)</title>
    <link href="http://github.com/YOLO.html"/>
    <id>http://github.com/YOLO.html</id>
    <published>2018-12-20T02:27:51.000Z</published>
    <updated>2019-01-04T23:33:06.334Z</updated>
    
    <content type="html"><![CDATA[<h1 id="YOLO-You-Only-Look-Once"><a href="#YOLO-You-Only-Look-Once" class="headerlink" title="YOLO: You Only Look Once"></a>YOLO: You Only Look Once</h1><p>YOLO是一种Real time的Object Detection的算法，他颠覆了当时两种主要的state-of-the-art的方法：sliding window &amp; region-proposal，YOLO使用回归的方法，将Image输入到一个CNN中直接输出检测到的Object的坐标和类别。因此testing time的时候只需要一次前馈即可，算法在保证较高精度的前提下实现了很高的计算速度（&gt;45 FPS，正常视频的帧数为20-30FPS）。Fast的计算速度带来的是Predict Location的较大误差和受困于小object，这是YOLO的主要难题。（对了，这里主要讨论YOLO v1）</p><h2 id="Main-Idea"><a href="#Main-Idea" class="headerlink" title="Main Idea"></a>Main Idea</h2><p>我们将图片分为S <em> S个grid（v1中使用7</em>7），每一个grid我们有B个Bounding Box（v1中B=2），每一个bounding box有5个参数：bounding box中心点坐标x，y和box的长宽w、h。需要注意的是这里的x、y、w、h都被归一化处理了，x&amp;y除以了grid location，w&amp;h除以了Image Size。</p><p>第5个参数对于这个bounding box中是否存在Object（先不管其class）的confidence：其中Pr(Object)=1 or 0，代表box中是否含有Object，IOU为其与ground truth的重合程度（我认为这里应该是和训练时的Object的重合程度吧= =）。<strong>这里计算的就是loss function里的ground truth Ci~</strong></p><p><img src="/images/YOLO/1.png" alt="img"></p><p>另外每一个grid会有C个class的条件概率预测，注意这里的条件概率每一个grid的B个bounding box共享。这里的条件概率和上述的confidence相乘，就能得到每一个box的各分类预测概率作为预测的主要依据。</p><p><img src="/images/YOLO/2.png" alt="img"></p><h2 id="Testing"><a href="#Testing" class="headerlink" title="Testing"></a>Testing</h2><p><img src="/images/YOLO/4.png" alt="img"></p><p>Test time的过程比较简单，图像输入CNN后会得到S <em> S </em> B个bounding box和其对应的class confidence，但是这里面大部分都不包含我们想要的Object，所以我们首先会有一个threshold(= 0.3 here)，confidence低于阈值的舍去，然后再做一个Non-maximal suppression（非极大值抑制）。</p><h3 id="Non-maximal-suppression（非极大值抑制）"><a href="#Non-maximal-suppression（非极大值抑制）" class="headerlink" title="Non-maximal suppression（非极大值抑制）"></a>Non-maximal suppression（非极大值抑制）</h3><p>非极大值抑制实际上几乎是所有Object detection算法在test time最后都会做的一件事，主要是为了避免对于同一个Object生成多个重合度很高的bounding box。过程也比较简单，对于每一个不同的class object（可能在一起也可能不在一起，never mind，只是一个稍微简化一点的过程）选出当前confidence最高的box，比较所有box和它的重合程度（IOU，就是计算两个box的重叠面积 / 选出的box的面积），IOU大于给定阈值即被舍去。迭代直到所有的box都被选出来为止。</p><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>每一篇Object detection论文的训练过程对于我这种不了解PASCAL VOC和COCO数据集内部结构的人简直是噩梦一样，但是总归看出了些东西，总结一下。</p><h3 id="Network"><a href="#Network" class="headerlink" title="Network"></a>Network</h3><p><img src="/images/YOLO/3.png" alt="img"></p><p>24层卷积层 + 2层FC层（之后YOLO v2取消了FC使用全卷积）。和大部分操作相同，先使用前20层在ImageNet上预训练，之后再加入后面的卷积层和全连接层。</p><h3 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h3><p>训练过程中，每一个正bounding box和ground truth是一一对应的。我们将标记好的ground truth box根据其中心点位置分配给某一个grid，再根据其B个box中哪一个和ground truth的IOU最高将这个ground truth分配个这个grid的这个box，剩下的没有分配ground truth的box都称为no object box。每一个box分配一个ground truth，i.e. 极限情况下如果一个box和两个ground truth都是理论上的最适组合也只能选一个，另一个给另一个bounding box。下图选自<a href="https://blog.csdn.net/c20081052/article/details/80236015" target="_blank" rel="noopener">某位同学的CSDN博客</a></p><p><img src="/images/YOLO/5.png" alt="img"></p><h3 id="Process"><a href="#Process" class="headerlink" title="Process"></a>Process</h3><p>训练的时候：输入N个图像，每个图像包含M个object，每个object包含4个坐标（x，y，w，h）和1个label。然后通过网络得到7 <em> 7 </em> 30大小的三维矩阵。每个1 * 30的向量前5个元素表示第一个bounding box的4个坐标和1个confidence，第6到10元素表示第二个bounding box的4个坐标和1个confidence。最后20个表示这个grid cell所属类别。注意这30个都是预测的结果。然后就可以计算损失函数的第一、二 、五行。至于第三四行，ground truth confidence可以根据ground truth和预测的bounding box计算出的IOU和是否有object的0,1值相乘得到。这样就能计算出loss function的值了。</p><p>同时location坐标x，y，w，h都被归一化，因此也可以认为是预测相对于grid坐标的修正，<strong>注意这里的定义和R-CNN的不同</strong>。</p><h2 id="tensorflow源码分析"><a href="#tensorflow源码分析" class="headerlink" title="tensorflow源码分析"></a>tensorflow源码分析</h2><p>主要就是分析一下detection的过程。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_build_detector</span><span class="params">(self)</span>:</span></span><br><span class="line">    self.width = tf.placeholder(tf.float32, name=<span class="string">"img_w"</span>)</span><br><span class="line">    self.height = tf.placeholder(tf.float32, name=<span class="string">"img_h"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># get class prob, confidence, boxes from net output</span></span><br><span class="line">    idx1 = self.S * self.S * self.C</span><br><span class="line">    idx2 = idx1 + self.S * self.S * self.B</span><br><span class="line">    <span class="comment"># class prediction</span></span><br><span class="line">    <span class="comment"># 这里axis=0的索引为0说明这是test time,只能够一次处理一张照片</span></span><br><span class="line">    class_probs = tf.reshape(self.predicts[<span class="number">0</span>, :idx1], [self.S, self.S, self.C])</span><br><span class="line">    <span class="comment"># confidence</span></span><br><span class="line">    confs = tf.reshape(self.predicts[<span class="number">0</span>, idx1:idx2], [self.S, self.S, self.B])</span><br><span class="line">    <span class="comment"># boxes -&gt; (x, y, w, h)</span></span><br><span class="line">    boxes = tf.reshape(self.predicts[<span class="number">0</span>, idx2:], [self.S, self.S, self.B, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># convert the x, y to the coordinates relative to the top left point of the image</span></span><br><span class="line">    <span class="comment"># the predictions of w, h are the square root</span></span><br><span class="line">    <span class="comment"># multiply the width and height of image</span></span><br><span class="line">    <span class="comment"># 可以看到这里直接预测值x&amp;y是相对于cell中心坐标的位移值</span></span><br><span class="line">    boxes = tf.stack([(boxes[:, :, :, <span class="number">0</span>] + tf.constant(self.x_offset, dtype=tf.float32)) / self.S * self.width,</span><br><span class="line">                      (boxes[:, :, :, <span class="number">1</span>] + tf.constant(self.y_offset, dtype=tf.float32)) / self.S * self.height,</span><br><span class="line">                      tf.square(boxes[:, :, :, <span class="number">2</span>]) * self.width,</span><br><span class="line">                      tf.square(boxes[:, :, :, <span class="number">3</span>]) * self.height], axis=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># class-specific confidence scores [S, S, B, C]</span></span><br><span class="line">    scores = tf.expand_dims(confs, <span class="number">-1</span>) * tf.expand_dims(class_probs, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    scores = tf.reshape(scores, [<span class="number">-1</span>, self.C])  <span class="comment"># [S*S*B, C]</span></span><br><span class="line">    boxes = tf.reshape(boxes, [<span class="number">-1</span>, <span class="number">4</span>])  <span class="comment"># [S*S*B, 4]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># find each box class, only select the max score</span></span><br><span class="line">    box_classes = tf.argmax(scores, axis=<span class="number">1</span>) <span class="comment"># 每个box的predict class, [98, 1]</span></span><br><span class="line">    box_class_scores = tf.reduce_max(scores, axis=<span class="number">1</span>) <span class="comment"># 每个box的最高scores值, [98, 1]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># filter the boxes by the score threshold</span></span><br><span class="line">    <span class="comment"># 删除scores低于阈值的项</span></span><br><span class="line">    filter_mask = box_class_scores &gt;= self.threshold</span><br><span class="line">    scores = tf.boolean_mask(box_class_scores, filter_mask) <span class="comment"># [N,]</span></span><br><span class="line">    boxes = tf.boolean_mask(boxes, filter_mask) <span class="comment"># [N, 4]</span></span><br><span class="line">    box_classes = tf.boolean_mask(box_classes, filter_mask) <span class="comment"># [N,]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># non max suppression (do not distinguish different classes)</span></span><br><span class="line">    <span class="comment"># ref: https://tensorflow.google.cn/api_docs/python/tf/image/non_max_suppression</span></span><br><span class="line">    <span class="comment"># box (x, y, w, h) -&gt; box (x1, y1, x2, y2)</span></span><br><span class="line">    _boxes = tf.stack([boxes[:, <span class="number">0</span>] - <span class="number">0.5</span> * boxes[:, <span class="number">2</span>], boxes[:, <span class="number">1</span>] - <span class="number">0.5</span> * boxes[:, <span class="number">3</span>],</span><br><span class="line">                       boxes[:, <span class="number">0</span>] + <span class="number">0.5</span> * boxes[:, <span class="number">2</span>], boxes[:, <span class="number">1</span>] + <span class="number">0.5</span> * boxes[:, <span class="number">3</span>]], axis=<span class="number">1</span>) <span class="comment"># # [N, 4]</span></span><br><span class="line">    nms_indices = tf.image.non_max_suppression(_boxes, scores,</span><br><span class="line">                                               self.max_output_size, self.iou_threshold)</span><br><span class="line">    self.scores = tf.gather(scores, nms_indices) <span class="comment"># [?,]</span></span><br><span class="line">    self.boxes = tf.gather(boxes, nms_indices) <span class="comment"># [?, 4]</span></span><br><span class="line">    self.box_classes = tf.gather(box_classes, nms_indices) <span class="comment"># [?,]</span></span><br></pre></td></tr></table></figure><ul><li><p>直接预测的坐标值实际上是相对于cell中心的位移值，长宽相对于整张图片，相对于SSD更容易理解。</p></li><li><p>先做score的阈值检查再做非极大值抑制，并没有在每个class空间下处理，导致的结果就是如果两个不同的object的重合度太高就不能够区分开来，如人拿着水杯，这是源码处理的一个取简之处，在运行过程中也有发现这个问题。</p></li><li><p><code>tf.image.non_max_suppression</code>非极大值抑制函数，真是厉害极了，查看官方API说明:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tf.image.non_max_suppression(</span><br><span class="line">    boxes,</span><br><span class="line">    scores,</span><br><span class="line">    max_output_size,</span><br><span class="line">    iou_threshold=<span class="number">0.5</span>,</span><br><span class="line">    score_threshold=float(<span class="string">'-inf'</span>),</span><br><span class="line">    name=<span class="keyword">None</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># 参数实际上都很好的理解，主要就是boxes的坐标表示为左上角坐标和右下角坐标的4维向量</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://arxiv.org/abs/1506.02640" target="_blank" rel="noopener">Paper</a></li><li><a href="https://www.youtube.com/watch?v=L0tzmv--CGY" target="_blank" rel="noopener">油管解析</a></li><li><a href="https://blog.csdn.net/xiaohu2022/article/details/79211732" target="_blank" rel="noopener">CSDN解析</a></li><li><a href="https://zhuanlan.zhihu.com/p/25053311" target="_blank" rel="noopener">源码解析</a></li><li><a href="https://github.com/hizhangp/yolo_tensorflow">python源代码</a></li></ol><h1 id="YOLO-v2-amp-YOLO9000"><a href="#YOLO-v2-amp-YOLO9000" class="headerlink" title="YOLO v2 &amp; YOLO9000"></a>YOLO v2 &amp; YOLO9000</h1><h2 id="YOLO-v2"><a href="#YOLO-v2" class="headerlink" title="YOLO v2"></a>YOLO v2</h2><p>YOLO最初版主要受限于：小物体的检测、定位的精度和低召回率。以此为目标YOLO v2在设计上进行了改进：</p><ul><li><p><strong>Batch Normalization</strong>：除了最后一层，每一层卷积层后面都加入了BN层</p></li><li><p><strong>High Resolution Classifier</strong>：由于检测需要高分辨率，因此这一步就是将网络改为输入为448*448的分类网络之后先直接在ImageNet上面做微调fine tune。使得分类器适应了高分辨率之后再在detection数据集上进行微调。</p></li><li><p><strong>Convolutional With Anchor Boxes</strong>：使用了archor box，更改输入为416*416，darknet19总的步长为32，因此最后使用的特征图尺度为13×13。这里作者提出<strong>最好使用奇数</strong>是因为这样在图片的中心点有一个cell，大物体更有可能落在图片的中心，有利于大物体的检测。</p><p>和YOLO v1不同的是，这里YOLO类似于SSD为每一个archor box都预测分类score向量（<strong>不包括背景类，但仍然是条件概率</strong>）和5个特征值（4个坐标和Pro(Object)）。使用archor明显的提升了recall。</p></li><li><p><strong>Dimension Clusters</strong>：RPN的先验框尺度是人工选择的，YOLOv2提出使用在所有训练集的边框当中做K-means选出聚类中心来选择更好的先验框尺度。这里由于我们希望我们的先验框能和ground truth有更大的IOU重合，所以这里我们不用欧式距离而使用IOU相对距离，定义如下：</p><p><img src="/images/YOLO/6.PNG" alt="img"></p><p>权衡复杂度和average IOU之后作者选择使用k=5个先验框，并记录聚类中心的dimension(长宽)</p></li><li><p><strong>Direct location prediction</strong>：RPN对于坐标的预测是预测框中心相对于对应的archor box中心的offset值，但是这个修正值的值域并没有限制，这个修正值可以很大从而导致预测框离archor很远，从而给训练带来了困难。因此YOLO v2沿用了YOLO的预测方法，预测值为<strong>预测框中心</strong>相对于cell左上角的位移，而且将位移值使用sigmoid函数修正值(0, 1)的范围；预测框长宽的预测方式不变。</p><p><img src="/images/YOLO/7.png" alt="img"></p><p>CSDN解析中讲到，由于之前的K-means都是在(13, 13)<strong>特征图</strong>上做的，所以这里的4个坐标值也都是特征图上的绝对坐标，实际使用的时候要进行线性变换，这样的道德四个坐标都是(0, 1)的数，也就不用care原图像的分辨率了。这一点我们将在源码分析中主要查看。</p><p><img src="/images/YOLO/8.png" alt="img"></p></li><li><p><strong>Fine-Grained Features</strong>：为了应对小物体的检测问题，学习SSD的多级特征图检测，将最后一个max pooling层的输入(26 × 26 × 512)，引用底层的特征图同时进行检测。代码中先使用shortcut层(1×1卷积)减小维度之后再转化为13×13的分辨率。（-&gt; 26×26×64 -&gt; 13×13×512，串联之后13×13×(1024+512)）。</p></li><li><p><strong>Multi-Scale Training</strong>：这一部分论文中没有过多阐述，详细可参看<a href="https://blog.csdn.net/xiaohu2022/article/details/80666655" target="_blank" rel="noopener">CSDN解析</a>。由于网络只有卷积和pooling层，所以理论上来说网络可以适用各种尺度的输入。再<strong>训练</strong>的时候，作者每隔10个batches就随机更新一种输入的分辨率，不同的输入分辨率都可以使用模型，只不过输出了不同尺度的最终级特征图，实际上对于特征图来说尺度只会影响最后的archor box的个数，对于操作来说并没有什么区别，但是通过不同分辨率的输入，让网络更好的适应了不同分辨率的图片输入。真是厉害极了！</p><p><img src="/images/YOLO/9.png" alt="img"></p></li><li><p><strong>Darknet-19训练过程</strong>：使用了FCN进行分类和检测。先使用Darknet-19在224×224的Imagenet上训练，然后再在448×448的Imagenet上训练，其中加入了典型的训练trick：data augmentation。之后移除最后一层1×1×1000的分类卷积，加入3个3×3×1024的卷积层，并将passthrough layer的输出输入到最后一层3×3卷积当中。最后接一层1×1的卷积做输出。<a href="http://ethereon.github.io/netscope/#/gist/d08a41711e48cf111e330827b1279c31" target="_blank" rel="noopener">网络可视化</a></p></li></ul><p><img src="/images/YOLO/10.png" alt="img"></p><ul><li>具体训练过程，包括损失函数，参看<a href="https://blog.csdn.net/xiaohu2022/article/details/80666655" target="_blank" rel="noopener">CSDN解析</a>，损失函数的分析很仔细。</li></ul><h2 id="YOLO9000"><a href="#YOLO9000" class="headerlink" title="YOLO9000"></a>YOLO9000</h2><p>YOLO实际上物体坐标的检测并不依赖于分类，所以作者认为可以将分类数据集（Imagenet）和检测数据集（COCO）联合训练以提升检测的物体数量。</p><ul><li><strong>Hierarchical classification</strong>：将wordnet的有向图结果转化为wordtree，将其中有多条路径的结点选择其中最短的一条添加到树结构中。这样卷积得到的预测值<strong>物体的置信度</strong>实际上就是<strong>Prob(Physical Object)</strong>，每一个结点都是一个条件概率。每次预测检查每一条path，选择概率最高的一条，一直往下直到条件概率的乘积低于阈值，将终点类作为当前的预测结果。</li><li><strong>Jointly Training</strong>：遇到检测数据集的数据就正常后向（包括objectness，bbox和classification loss），但是如果遇到分类数据的话，就粗略地找到一个bbox然后以此作为“标记好的”bbox后向（只包括classification和objectness loss，没有bbox），其中将IOU（即<strong>Prob(Physical Object)</strong>）设为0.3。</li></ul><h2 id="tensorflow源码分析-1"><a href="#tensorflow源码分析-1" class="headerlink" title="tensorflow源码分析"></a>tensorflow源码分析</h2><ul><li>先来看一下卷积层的定义</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, filters, size, pad=<span class="number">0</span>, stride=<span class="number">1</span>, batch_normalize=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">           activation=leaky_relu, use_bias=False, name=<span class="string">"conv2d"</span>)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> pad &gt; <span class="number">0</span>:</span><br><span class="line">        x = tf.pad(x, [[<span class="number">0</span>, <span class="number">0</span>], [pad, pad], [pad, pad], [<span class="number">0</span>, <span class="number">0</span>]])</span><br><span class="line">    <span class="comment"># 在BN层之后输入激活函数</span></span><br><span class="line">    out = tf.layers.conv2d(x, filters, size, strides=stride, padding=<span class="string">"VALID"</span>,</span><br><span class="line">                           activation=<span class="keyword">None</span>, use_bias=use_bias, name=name)</span><br><span class="line">    <span class="keyword">if</span> batch_normalize == <span class="number">1</span>:</span><br><span class="line">        out = tf.layers.batch_normalization(out, axis=<span class="number">-1</span>, momentum=<span class="number">0.9</span>,</span><br><span class="line">                                            training=<span class="keyword">False</span>, name=name+<span class="string">"_bn"</span>)</span><br><span class="line">    <span class="keyword">if</span> activation:</span><br><span class="line">        out = activation(out)</span><br><span class="line">    <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><p>可以发现在每一个卷积层（除了最后的输出卷积层）在激活函数之前都加入了BN层，控制训练防止过拟合</p><ul><li>再看一下decode的过程</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(detection_feat, feat_sizes=<span class="params">(<span class="number">13</span>, <span class="number">13</span>)</span>, num_classes=<span class="number">80</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">           anchors=None)</span>:</span></span><br><span class="line">    H, W = feat_sizes <span class="comment"># (13, 13)</span></span><br><span class="line">    num_anchors = len(anchors) <span class="comment"># 5</span></span><br><span class="line">    <span class="comment"># [1, 13, 13, 425] -&gt; [1, 13*13, 5, 85]</span></span><br><span class="line">    detetion_results = tf.reshape(detection_feat, [<span class="number">-1</span>, H * W, num_anchors,</span><br><span class="line">                                        num_classes + <span class="number">5</span>])</span><br><span class="line"></span><br><span class="line">    bbox_xy = tf.nn.sigmoid(detetion_results[:, :, :, <span class="number">0</span>:<span class="number">2</span>])</span><br><span class="line">    bbox_wh = tf.exp(detetion_results[:, :, :, <span class="number">2</span>:<span class="number">4</span>])</span><br><span class="line">    obj_probs = tf.nn.sigmoid(detetion_results[:, :, :, <span class="number">4</span>])</span><br><span class="line">    class_probs = tf.nn.softmax(detetion_results[:, :, :, <span class="number">5</span>:])</span><br><span class="line"></span><br><span class="line">    anchors = tf.constant(anchors, dtype=tf.float32) <span class="comment"># [5, 2]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 这里的height和width都是特征图的尺度</span></span><br><span class="line">    height_ind = tf.range(H, dtype=tf.float32)</span><br><span class="line">    width_ind = tf.range(W, dtype=tf.float32)</span><br><span class="line">    x_offset, y_offset = tf.meshgrid(height_ind, width_ind)</span><br><span class="line">    x_offset = tf.reshape(x_offset, [<span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>])</span><br><span class="line">    y_offset = tf.reshape(y_offset, [<span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># decode</span></span><br><span class="line">    bbox_x = (bbox_xy[:, :, :, <span class="number">0</span>] + x_offset) / W</span><br><span class="line">    bbox_y = (bbox_xy[:, :, :, <span class="number">1</span>] + y_offset) / H</span><br><span class="line">    <span class="comment"># 这里除以2是为了下面不用除了</span></span><br><span class="line">    bbox_w = bbox_wh[:, :, :, <span class="number">0</span>] * anchors[:, <span class="number">0</span>] / W * <span class="number">0.5</span></span><br><span class="line">    bbox_h = bbox_wh[:, :, :, <span class="number">1</span>] * anchors[:, <span class="number">1</span>] / H * <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">    bboxes = tf.stack([bbox_x - bbox_w, bbox_y - bbox_h,</span><br><span class="line">                       bbox_x + bbox_w, bbox_y + bbox_h], axis=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># bboxes: [1, 13*13, 5, 4]</span></span><br><span class="line">    <span class="comment"># obj_probs: [1, 13*13, 5, 1]</span></span><br><span class="line">    <span class="comment"># class_probs: [1, 13*13, 5, 80]</span></span><br><span class="line">    <span class="keyword">return</span> bboxes, obj_probs, class_probs</span><br></pre></td></tr></table></figure><p>过程相对SSD来说就非常直接了，真正的是预测的是offset的值。</p><ul><li>来看一下YOLO的图像预处理</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_image</span><span class="params">(image, image_size=<span class="params">(<span class="number">416</span>, <span class="number">416</span>)</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Preprocess a image to inference"""</span></span><br><span class="line">    image_cp = np.copy(image).astype(np.float32)</span><br><span class="line">    <span class="comment"># resize the image</span></span><br><span class="line">    image_rgb = cv2.cvtColor(image_cp, cv2.COLOR_BGR2RGB)</span><br><span class="line">    image_resized = cv2.resize(image_rgb, image_size)</span><br><span class="line">    <span class="comment"># normalize</span></span><br><span class="line">    image_normalized = image_resized.astype(np.float32) / <span class="number">255.0</span> <span class="comment"># [0, 1]</span></span><br><span class="line">    <span class="comment"># expand the batch_size dim</span></span><br><span class="line">    image_expanded = np.expand_dims(image_normalized, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> image_expanded</span><br></pre></td></tr></table></figure><p>可以发现YOLO的预处理不需要均值归0，即最终的图像数据的取值范围为(0, 1)，而SSD是改至(-1, 1)的。</p><ul><li>看一下loss函数（和上面的demo应该不是一起的，有一些细节上有出入）</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_loss</span><span class="params">(predictions, targets, anchors, scales, num_classes=<span class="number">20</span>, feat_sizes=<span class="params">(<span class="number">13</span>, <span class="number">13</span>)</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the loss of Yolov2 for training</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    H, W = feat_sizes</span><br><span class="line">    C = num_classes</span><br><span class="line">    B = len(anchors)</span><br><span class="line">    anchors = tf.constant(anchors, dtype=tf.float32)</span><br><span class="line">    anchors = tf.reshape(anchors, [<span class="number">1</span>, <span class="number">1</span>, B, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">    sprob, sconf, snoob, scoor = scales  <span class="comment"># the scales for different parts</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 实际上可以根据_confs来判断，如果是1那么_coords和_probs的值有效</span></span><br><span class="line">    <span class="comment"># 否则无效，也有可能是0吧</span></span><br><span class="line">    _coords = targets[<span class="string">"coords"</span>]  <span class="comment"># ground truth [-1, H*W, B, 4]</span></span><br><span class="line">    _probs = targets[<span class="string">"probs"</span>]    <span class="comment"># class probability [-1, H*W, B, C] one hot，对于YOLO来说background不算在里面</span></span><br><span class="line">    _confs = targets[<span class="string">"confs"</span>]    <span class="comment"># 1 for object, 0 for background, [-1, H*W, B]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># decode the net output</span></span><br><span class="line">    predictions = tf.reshape(predictions, [<span class="number">-1</span>, H, W, B, (<span class="number">5</span> + C)])</span><br><span class="line">    coords = predictions[:, :, :, :, <span class="number">0</span>:<span class="number">4</span>]   <span class="comment"># t_x, t_y, t_w, t_h</span></span><br><span class="line">    coords = tf.reshape(coords, [<span class="number">-1</span>, H*W, B, <span class="number">4</span>])</span><br><span class="line">    <span class="comment"># x,y,w,h全部落在了0-1之间</span></span><br><span class="line">    coords_xy = tf.nn.sigmoid(coords[:, :, :, <span class="number">0</span>:<span class="number">2</span>])  <span class="comment"># (0, 1) relative cell top left</span></span><br><span class="line">    coords_wh = tf.sqrt(tf.exp(coords[:, :, :, <span class="number">2</span>:<span class="number">4</span>]) * anchors /</span><br><span class="line">                        np.reshape([W, H], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>])) <span class="comment"># sqrt of w, h (0, 1)</span></span><br><span class="line">    coords = tf.concat([coords_xy, coords_wh], axis=<span class="number">3</span>)  <span class="comment"># [batch_size, H*W, B, 4]</span></span><br><span class="line"></span><br><span class="line">    confs = tf.nn.sigmoid(predictions[:, :, :, :, <span class="number">4</span>])  <span class="comment"># object confidence</span></span><br><span class="line">    confs = tf.reshape(confs, [<span class="number">-1</span>, W * H, B, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    probs = tf.nn.softmax(predictions[:, :, :, :, <span class="number">5</span>:])  <span class="comment"># class probability</span></span><br><span class="line">    probs = tf.reshape(probs, [<span class="number">-1</span>, H*W, B, C])</span><br><span class="line"></span><br><span class="line">    preds = tf.concat([coords, confs, probs], axis=<span class="number">3</span>)  <span class="comment"># [-1, H*W, B, (4+1+C)]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># match ground truths with anchors (predictions in fact)</span></span><br><span class="line">    <span class="comment"># assign ground truths to the predictions with the best IOU (select 1 among 5 anchors)</span></span><br><span class="line">    <span class="comment"># 应该是和YOLO v1相同将w和h取了平方根</span></span><br><span class="line">    wh = tf.pow(coords[:, :, :, <span class="number">2</span>:<span class="number">4</span>], <span class="number">2</span>) * np.reshape([W, H], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">    areas = wh[:, :, :, <span class="number">0</span>] * wh[:, :, :, <span class="number">1</span>]</span><br><span class="line">    centers = coords[:, :, :, <span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line">    <span class="comment"># 左上点和右下点的坐标</span></span><br><span class="line">    up_left, down_right = centers - (wh * <span class="number">0.5</span>), centers + (wh * <span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># the ground truth</span></span><br><span class="line">    _wh = tf.pow(_coords[:, :, :, <span class="number">2</span>:<span class="number">4</span>], <span class="number">2</span>) * np.reshape([W, H], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">    _areas = _wh[:, :, :, <span class="number">0</span>] * _wh[:, :, :, <span class="number">1</span>]</span><br><span class="line">    _centers = _coords[:, :, :, <span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line">    _up_left, _down_right = _centers - (_wh * <span class="number">0.5</span>), _centers + (_wh * <span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute IOU</span></span><br><span class="line">    inter_upleft = tf.maximum(up_left, _up_left)</span><br><span class="line">    inter_downright = tf.minimum(down_right, _down_right)</span><br><span class="line">    inter_wh = tf.maximum(inter_downright - inter_upleft, <span class="number">0.0</span>)</span><br><span class="line">    intersects = inter_wh[:, :, :, <span class="number">0</span>] * inter_wh[:, :, :, <span class="number">1</span>]</span><br><span class="line">    ious = tf.truediv(intersects, areas + _areas - intersects)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># axis = 2,是在B这个维度上来说的，也就是说对于feature map上的这个cell的B个archor box选择一个IOU最大的</span></span><br><span class="line">    <span class="comment"># 也就是说YOLO仍然是将cell和gt匹配，而不是archor和gt匹配</span></span><br><span class="line">    best_iou_mask = tf.equal(ious, tf.reduce_max(ious, axis=<span class="number">2</span>, keep_dims=<span class="keyword">True</span>))</span><br><span class="line">    best_iou_mask = tf.cast(best_iou_mask, tf.float32)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 经过这次计算，所有有ground truth的archor的预测bbox和gt的bbox的IOU都得到了</span></span><br><span class="line">    mask = best_iou_mask * _confs  <span class="comment"># [-1, H*W, B]</span></span><br><span class="line">    mask = tf.expand_dims(mask, <span class="number">-1</span>)  <span class="comment"># [-1, H*W, B, 1]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute weight terms</span></span><br><span class="line">    <span class="comment"># [-1, H*W, B, 1]</span></span><br><span class="line">    confs_w = snoob * (<span class="number">1</span> - mask) + sconf * mask</span><br><span class="line">    coords_w = scoor * mask</span><br><span class="line">    probs_w = sprob * mask</span><br><span class="line">    weights = tf.concat([coords_w, confs_w, probs_w], axis=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    truths = tf.concat([_coords, tf.expand_dims(_confs, <span class="number">-1</span>), _probs], <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    loss = tf.pow(preds - truths, <span class="number">2</span>) * weights</span><br><span class="line">    loss = tf.reduce_sum(loss, axis=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">    loss = <span class="number">0.5</span> * tf.reduce_mean(loss)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><ul><li>主要思路还是很简单，将gt和prediction 使用相同的方式进行解码之后做L2 loss，仍然是一个典型的回归模型，但是这里的权重生成过程很迷，但是其中很大程度上都依赖于<strong>predict box和gt的IOU值</strong>。</li><li>还有和SSD很大的不同是，YOLO仍然是将gt和cell相匹配（即使是在特征图上），一个cell之匹配一个gt，一个gt也只匹配一个cell，所以loss在解码之前，先选择了每个cell的B个predict box和对应的gt的IOU最大的那一个框进行loss的计算。</li><li>还有就是和YOLO v1相同的一点是，计算loss的时候将w和h先取了平方根。</li></ul><h2 id="Reference-1"><a href="#Reference-1" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://blog.csdn.net/xiaohu2022/article/details/80666655" target="_blank" rel="noopener">CSDN解析</a></li><li><a href="https://towardsdatascience.com/training-object-detection-yolov2-from-scratch-using-cyclic-learning-rates-b3364f7e4755" target="_blank" rel="noopener">Training 过程解析</a></li><li><a href="https://docs.google.com/presentation/d/14qBAiyhMOFl_wZW4dA1CkixgXwf0zKGbpw_0oHK8yEM/edit#slide=id.p" target="_blank" rel="noopener">YOLO9000 talk</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;YOLO-You-Only-Look-Once&quot;&gt;&lt;a href=&quot;#YOLO-You-Only-Look-Once&quot; class=&quot;headerlink&quot; title=&quot;YOLO: You Only Look Once&quot;&gt;&lt;/a&gt;YOLO: You Only L
      
    
    </summary>
    
      <category term="Computer Science" scheme="http://github.com/categories/Computer-Science/"/>
    
      <category term="Computer Vision" scheme="http://github.com/categories/Computer-Science/Computer-Vision/"/>
    
    
      <category term="Object Detection" scheme="http://github.com/tags/Object-Detection/"/>
    
      <category term="Paper Reading" scheme="http://github.com/tags/Paper-Reading/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow Notes</title>
    <link href="http://github.com/Tensorflow.html"/>
    <id>http://github.com/Tensorflow.html</id>
    <published>2018-12-16T14:36:42.000Z</published>
    <updated>2019-01-01T17:33:49.040Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Tensorflow"><a href="#Tensorflow" class="headerlink" title="Tensorflow"></a>Tensorflow</h1><p><a href="https://www.tensorflow.org/tutorials/" target="_blank" rel="noopener">Tensorflow</a> 已经毋庸置疑地成为了目前最火热的深度学习framework，相信其中也有不少Google的因素在里面。通过<em>CS231n</em>的Assignment我也正式入坑了TF，这篇博文就将专门用来记录我在使用tensorflow的过程当中遇到了一个Bug和比较厉害的API，因此这篇文章也将长期更新！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow.contrib.eager <span class="keyword">as</span> tfe</span><br></pre></td></tr></table></figure><h1 id="1-Tensorboard"><a href="#1-Tensorboard" class="headerlink" title="1. Tensorboard"></a>1. Tensorboard</h1><p>​Tensorboard是Tensorflow自带的一个可视化工具，你需要在Python中添加指定的指令就可以帮助你可视化计算图以及某些变量（e.g. loss &amp; accuracy）的变化情况。所有的信息存储在一个<code>event</code>文件当中。</p><h2 id="Open-Tensorboard"><a href="#Open-Tensorboard" class="headerlink" title="Open Tensorboard"></a>Open Tensorboard</h2><p>打开tensorboard的方式和jupyter notebook的方式差不多，你需要进入<code>event</code>文件的上上层目录，即如果<code>event</code>文件存储在<strong>“/tensorboard/graphs/xx. event”</strong>目录下，你就要先进入<strong>tensorboard</strong>，然后运行指令：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir=<span class="string">"./graphs"</span></span><br></pre></td></tr></table></figure><p>即可。实际操作的时候可能会经常出现Bug，我至今没有搞清楚是为什么，因为经常相同的文件相同的指令，我多次打开tensorboard有的时候找得到event文件有的时候找不到，可能是因为TF的版本太低了吧 = = Anyway</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><ol><li><p>记录计算图，在你的所有的计算图定义结束了之后加入指令：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">writer = tf.summary.FileWriter(<span class="string">'./graphs'</span>, tf.get_default_graph())</span><br></pre></td></tr></table></figure><p>即可。当然你可以更改后面的参数记录其他子图的定义。</p></li><li><p>记录某个变量的训练过程有scalar、histogram等多种方式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tf.summary.scalar(<span class="string">"loss"</span>, loss)</span><br><span class="line">tf.summary.histogram(<span class="string">"histogram loss"</span>, accuracy)</span><br><span class="line">summary_op = tf.summary.merge_all()</span><br><span class="line"></span><br><span class="line">With tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line"><span class="keyword">for</span> index <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">        summary = sess.run([summary_op])</span><br><span class="line">        writer.add_summary(summary, global_step=index)</span><br></pre></td></tr></table></figure><p>在这里值得说明的是，有多个summary操作的话需要将summary操作merge之后添加到writer当中，否则是不会成功显示的。<br>另外就是在这里的<code>global_step</code>这个参数很重要，是全局索引，在optimizer中也会用到。</p></li></ol><h1 id="2-tf-train-Saver"><a href="#2-tf-train-Saver" class="headerlink" title="2. tf.train.Saver()"></a>2. tf.train.Saver()</h1><p>参数存储。需要和checkpoint文件一起存储作为参数的map。当然你也可以指定需要存储的特定参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 存储过程</span></span><br><span class="line">saver = tf.train.Saver() </span><br><span class="line">saver.save(sess, store path, global_step)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 先restore checkpoint，里面有变量名的索引，然后再restore值文件</span></span><br><span class="line">ckpt = tf.train.get_checkpoint_state(os.path.dirname(<span class="string">'path'</span>)) </span><br><span class="line">saver.restore(sess, path)</span><br></pre></td></tr></table></figure><h1 id="tf-cond-pred-fn1-fn2-name-None"><a href="#tf-cond-pred-fn1-fn2-name-None" class="headerlink" title="tf.cond(pred, fn1, fn2, name=None)"></a>tf.cond(pred, fn1, fn2, name=None)</h1><p>没有eager mode的情况下不能使用python if / for/ while，因为这时候tensor里面没有实际的值，这时候需要使用特定的分支API，其中fn1和fn2是函数，看下面的例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">huber_loss</span><span class="params">(labels, predictions, delta=<span class="number">14.0</span>)</span>:</span> </span><br><span class="line"> residual = tf.abs(labels - predictions) </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">f1</span><span class="params">()</span>:</span> <span class="keyword">return</span> <span class="number">0.5</span> * tf.square(residual) </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">f2</span><span class="params">()</span>:</span> <span class="keyword">return</span> delta * residual - <span class="number">0.5</span> * tf.square(delta) </span><br><span class="line">  <span class="keyword">return</span> tf.cond(residual &lt; delta, f1, f2)</span><br></pre></td></tr></table></figure><h1 id="3-tf-data"><a href="#3-tf-data" class="headerlink" title="3. tf.data"></a>3. tf.data</h1><h2 id="tf-data-Dataset"><a href="#tf-data-Dataset" class="headerlink" title="tf.data.Dataset()"></a>tf.data.Dataset()</h2><p>实验证明，使用这种方式比使用placeholder在速度上会有显著的提升，之后使用Iterator来访问，当然tf.data.Dataset()也可以设置<code>batch_size</code>。生成API的方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 注意这是实际上只有一个Input</span></span><br><span class="line">dataset = tf.data.Dataset.from_tensor_slices((features, labels)) <span class="comment"># 参数实际上是一个二元组</span></span><br><span class="line">dataset = dataset.shuffle(<span class="number">10000</span>) <span class="comment"># if you want to shuffle your data</span></span><br><span class="line">dataset = dataset.batch(batch_size)</span><br></pre></td></tr></table></figure><h2 id="tf-Iterator"><a href="#tf-Iterator" class="headerlink" title="tf.Iterator()"></a>tf.Iterator()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. Iterates through the dataset exactly once. No need to initialization. </span></span><br><span class="line">iterator = dataset.make_one_shot_iterator() </span><br><span class="line">X, Y = iterator.get_next() <span class="comment"># 获取一组数据的方法</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Iterates through the dataset as many times as we want. Need to initialize with each epoch. </span></span><br><span class="line">iterator = dataset.make_initializable_iterator() </span><br><span class="line">sess.run(iterator.initializer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 一个普适class</span></span><br><span class="line">iterator = tf.data.Iterator.from_structure(train_data.output_types, train_data.output_shapes) </span><br><span class="line"><span class="comment"># 为了应对训练集和测试集的情况，不指定某个特定dataset的迭代器，而是指定了一种结构，具体dataset的指向在初始化的时候完成，就是相同的结构给训练集和测试集同样分配一个迭代器 </span></span><br><span class="line">train_init = iterator.make_initializer(train_data) <span class="comment"># initializer for train_data </span></span><br><span class="line">test_init = iterator.make_initializer(test_data) <span class="comment"># initializer for train_data</span></span><br></pre></td></tr></table></figure><h1 id="4-Eager-Model"><a href="#4-Eager-Model" class="headerlink" title="4. Eager Model"></a>4. Eager Model</h1><p>真的是，在我无比适应计算图之后告诉我eager model才是未来的趋势真的是令人心碎，Eager model使得tensor像numpy array一样易操作，支持各种分支和索引。但是缺点就是一旦开启无法关闭，这个还是很不走心的，尤其是在Eager model和底层TF API兼容性那么差的情况下。不过在这里很高兴的是：<strong>Keras支持Eager model！</strong></p><ul><li><code>tfe.Variable</code></li><li><code>tfe.gradient_function(f)</code> 求给定函数f的输出对于输入的梯度</li><li><code>tfe.implicit_gradients(f)</code> 求给定函数f对于在计算过程中涉及到的tfe.Variable的梯度</li><li><code>tf.train.AdamOptimizer().apply_gradients(grad)</code> Eager model给我们平常使用的<code>tf.train.AdamOptimizer().minimize(loss)</code>也带来了影响，因为minimize的loss参数必须是一个函数，是一个tensorflow op，但是在eager model下我们计算得到的loss往往就是一个和numpy可以看成相同的tensor，所以原来的API无法继续使用，改为使用以grad为参数的新的API，值得注意的是apply_gradient就是minimize的第二步操作</li></ul><h1 id="5-API-details"><a href="#5-API-details" class="headerlink" title="5. API details"></a>5. API details</h1><ol><li>Name Scope  VS Variable Scope</li></ol><ul><li>with tf.name_scope(‘two_layers’) as scope:<br>一方面是为了简化计算图，另一方面使得不同name scope中可以使用相同名字<strong>（这里的名字Python Varaiable的命名）</strong>。<br>E.g. <strong>GANs</strong>中我们就可以在两个网络下使用相同的参数名字。</li><li>with tf.variable_scope(‘two_layers’) as scope:<br>Variable Scope的使用促进了参数的share。看下面的例子即可。注意reuse有两个前提：<ol><li>重复使用的参数具有相同的名字，这里的名字指的是<strong>计算图中的name</strong></li><li>scope.reuse_variables()</li></ol></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'two_layers'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    logits1 = two_hidden_layers(x1)</span><br><span class="line">    scope.reuse_variables()</span><br><span class="line">    logits2 = two_hidden_layers(x2)</span><br></pre></td></tr></table></figure><ol start="2"><li>Tensorflow Padding</li></ol><ul><li>Valid = no padding：使用valid的时候实际上就是没有padding，卷积核从头开始slide，如果最后一侧滑动没有到最后一个元素，但是已经不够了，就舍弃剩下的。</li><li>SAME = padding：same情况下如果stride=1的时候输入输出的长宽相同。但是值得注意的是tensorflow的padding和Caffe不同，tensorflow优先选择在activation map的右边填充0，而caffe优先在左边padding</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Tensorflow&quot;&gt;&lt;a href=&quot;#Tensorflow&quot; class=&quot;headerlink&quot; title=&quot;Tensorflow&quot;&gt;&lt;/a&gt;Tensorflow&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://www.tensorflow.org/tu
      
    
    </summary>
    
      <category term="Computer Science" scheme="http://github.com/categories/Computer-Science/"/>
    
      <category term="Deep Learning" scheme="http://github.com/categories/Computer-Science/Deep-Learning/"/>
    
    
      <category term="Tensorflow" scheme="http://github.com/tags/Tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>博客搭建</title>
    <link href="http://github.com/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA.html"/>
    <id>http://github.com/博客搭建.html</id>
    <published>2018-12-15T14:59:34.000Z</published>
    <updated>2019-06-27T10:04:25.038Z</updated>
    
    <content type="html"><![CDATA[<p>花了一天时间总算是搭好了Hexo + Next主题的个人博客，在这里简单介绍一些搭建的过程。</p><h1 id="1-安装Node-js和Git"><a href="#1-安装Node-js和Git" class="headerlink" title="1. 安装Node.js和Git"></a>1. 安装<em>Node.js</em>和<em>Git</em></h1><p>Hexo基于<em>Node.js</em>和<em>Git</em>，所以为了方便使用需要提前下载这两个东西，网上教程很多，而且实际上只要到官网上下载安装包安装即可，过程还是非常简单的。这里特别提一点：在git首先安装完成后，需要设置user的‘name’和‘email’，如果有Github账户需要和Github的相关信息相同。在Git Bash运行如下指令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git config --global user.name <span class="string">"Your Name"</span></span><br><span class="line">$ git config --global user.email <span class="string">"email@example.com"</span></span><br></pre></td></tr></table></figure><h1 id="2-安装Hexo"><a href="#2-安装Hexo" class="headerlink" title="2. 安装Hexo"></a>2. 安装Hexo</h1><p>查看<a href="https://hexo.io/zh-cn/docs/" target="_blank" rel="noopener">Hexo官方指南</a>。首先安装Hexo</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ npm install -g hexo-cli</span><br><span class="line">$ hexo init blog</span><br></pre></td></tr></table></figure><p>安装成功之后进入任意你指定的Blog文件夹，在这个文件夹中初始化网站配置。使用hexo generator生成html文件，打开hexo server在本地预览网站。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> blog</span><br><span class="line">$ npm install</span><br><span class="line">$ hexo generator</span><br><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><h1 id="3-使用Next主题"><a href="#3-使用Next主题" class="headerlink" title="3. 使用Next主题"></a>3. 使用Next主题</h1><p>到<a href="https://github.com/iissnan/hexo-theme-next">Next的Github主页</a>下载源代码并放到Blog目录下的<code>themes/next</code>目录下。Next是一个干净整洁并且支持多种个性化操作的主题，相关配置可以参考一下两篇博文：</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://www.jianshu.com/p/9f0e90cc32c2" target="_blank" rel="noopener">Hexo-NexT配置超炫网页效果</a></li><li><a href="https://reuixiy.github.io/technology/computer/computer-aided-art/2017/06/09/hexo-next-optimization.html#fn:2" target="_blank" rel="noopener">打造个性超赞博客Hexo+NexT+GitHubPages的超深度优化</a></li></ul><h1 id="4-发布到Github-io或者是各种服务器上"><a href="#4-发布到Github-io或者是各种服务器上" class="headerlink" title="4. 发布到Github.io或者是各种服务器上"></a>4. 发布到Github.io或者是各种服务器上</h1><p>这里推荐使用SSH公钥系统来链接本地Git和Github账户，博主因为忘记了Github密码而尝试了十几次错误密码= =。这里特别指出的一点是在deploy之前必须要save git，否则会导致本地网站正确预览，但是部署到外部服务器(e.g. Github.io)后出现问题。运行下面这条命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure><p>具体操作可以参考下面的视频：</p><iframe width="300" height="180" src="https://www.youtube.com/embed/2rZsV-Frf2E" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><h1 id="5-博文写作"><a href="#5-博文写作" class="headerlink" title="5. 博文写作"></a>5. 博文写作</h1><p>现在你可以开始攥写你的博文啦！Hexo支持Markdown编辑，大大的刺激了程序员的写作热情。Next对于Markdown也有多种小而实用的插件支持，感兴趣的话可以参考<a href="http://theme-next.iissnan.com/" target="_blank" rel="noopener">Next官方文档</a> </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;花了一天时间总算是搭好了Hexo + Next主题的个人博客，在这里简单介绍一些搭建的过程。&lt;/p&gt;
&lt;h1 id=&quot;1-安装Node-js和Git&quot;&gt;&lt;a href=&quot;#1-安装Node-js和Git&quot; class=&quot;headerlink&quot; title=&quot;1. 安装Nod
      
    
    </summary>
    
      <category term="Computer Science" scheme="http://github.com/categories/Computer-Science/"/>
    
      <category term="Technique" scheme="http://github.com/categories/Computer-Science/Technique/"/>
    
    
      <category term="hexo" scheme="http://github.com/tags/hexo/"/>
    
      <category term="Next" scheme="http://github.com/tags/Next/"/>
    
      <category term="Blog Build" scheme="http://github.com/tags/Blog-Build/"/>
    
  </entry>
  
  <entry>
    <title>Summary for CS231n</title>
    <link href="http://github.com/CS231n.html"/>
    <id>http://github.com/CS231n.html</id>
    <published>2018-12-15T08:25:08.000Z</published>
    <updated>2018-12-16T10:45:31.695Z</updated>
    
    <content type="html"><![CDATA[<h2 id="CS231n"><a href="#CS231n" class="headerlink" title="CS231n"></a>CS231n</h2><p><a href="http://cs231n.stanford.edu/" target="_blank" rel="noopener">CS231n Official Site</a><br>2018年12月12日，我总算是完成了CS231n所有的lecture和assignment。现在从头算来，断断续续的用了5个月的时间才全部搞定（当然由于我使用的深度学习框架为Tensorflow，所以和Pytorch相关的作业都没有写hhh)。 总的来说真的只能用神课来形容，作为一门深度学习和计算机视觉入门的基础课，真的是做到了由浅至深，先做好了DNN (CNN especially here)的基础教学，再深入到其在视觉领域当中的应用，课程过程当中有很多有趣的paper和demo，真的是让人在不经意之间会中了视觉的毒。 Anyway，墙裂推荐刚刚开始学习machine learning和AI的朋友们认真听一下这门课，可以帮助我们打好基础。</p><h2 id="Lecture"><a href="#Lecture" class="headerlink" title="Lecture"></a>Lecture</h2><p>这门课的Lecture充分体现了这是一门入门课的特点，由浅至深，同时带来的结果就是前面讲的很仔细但是到了后期的应用部分就显得有些仓促了（或许是因为被前半部分养刁了耳朵不吧hhh）<br>CS231n首先以Imagenet Image Classification为引，花了半个学期的时间详细讲解了CNN (architecture) &amp; 训练DNN的方法，包括正则化（Dropout, Batch Normalization, 翻转和引入random noise）和各种optimizer(Adam, RMSProp……)等等。下半个学期过渡到Vision的实际问题当中，包括了Image Captionin, Segmentation &amp; detection, visualization, Generative Model and Reinforcement Learning。介绍了计算机视觉的3个主要的研究方向，至少对这个领域在研究的大问题有了了解，但是涉及到的model都没有很困难，有利于大家深刻理解这堂课上讲的内容。</p><p>另外这里值得一提的是，因为我自己也上过吴恩达的Machine Learning @ Coursera，两者相比就产生了很打的差别，导致我再也没听过Coursera的课。Coursera上的课和Stanford的公开课相比可以说是入门课的普及课，两者不论在授课节奏上还是课程要求上都存在不小的差别（可惜的是时间上却差不多= =），所以我推荐大家有资源的话还是尽可能听公开课，MOOC慎听！</p><h2 id="Notes-amp-Assignments"><a href="#Notes-amp-Assignments" class="headerlink" title="Notes &amp; Assignments"></a>Notes &amp; Assignments</h2><p>这门课另外一个吸粉点就是他强大的TA团队。众所周知CS231n的Lecture Notes质量是非常高的，在总结lecture内容的基础上进行了适当的衍生，同时有很多背景和术语上的详细解释（我想如果Google Scholar对CS231n的Notes进行引用统计的话或许现在已经破千了吧）。</p><p>这门课的Assignment是真的，让人产生一种很想写作业的冲动，虽然debug过程仍然是让人痛苦得想摔电脑。但是这3个assignment确实让我们了解了CNN、RNN和涉及到的model在实现过程中的很多细节问题，与此同时还让我充分练习了我的Python和Tensforflow的代码能力。</p><p>这里我也贴上我自己的github Assignment链接供大家参考: <a href="https://github.com/KaiChen1998/CS231n-1718spring">CS231n 17-18 Spring</a></p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Thanks for all teaching staffs!<br>强烈推荐！这是一门值得仔细听的好课。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;CS231n&quot;&gt;&lt;a href=&quot;#CS231n&quot; class=&quot;headerlink&quot; title=&quot;CS231n&quot;&gt;&lt;/a&gt;CS231n&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;http://cs231n.stanford.edu/&quot; target=&quot;_blank&quot; 
      
    
    </summary>
    
      <category term="Computer Science" scheme="http://github.com/categories/Computer-Science/"/>
    
      <category term="Computer Vision" scheme="http://github.com/categories/Computer-Science/Computer-Vision/"/>
    
    
      <category term="Deep Learning" scheme="http://github.com/tags/Deep-Learning/"/>
    
      <category term="Computer Vision" scheme="http://github.com/tags/Computer-Vision/"/>
    
  </entry>
  
  <entry>
    <title>Kiku_Ju</title>
    <link href="http://github.com/Kiku%20Ju.html"/>
    <id>http://github.com/Kiku Ju.html</id>
    <published>2018-12-15T06:01:28.000Z</published>
    <updated>2018-12-16T00:24:48.648Z</updated>
    
    <content type="html"><![CDATA[<h1 id="开博第一帖献给我鞠，希望大家可以喜欢上这个努力的女孩子"><a href="#开博第一帖献给我鞠，希望大家可以喜欢上这个努力的女孩子" class="headerlink" title="开博第一帖献给我鞠，希望大家可以喜欢上这个努力的女孩子"></a>开博第一帖献给我鞠，希望大家可以喜欢上这个努力的女孩子</h1><iframe src="//player.bilibili.com/player.html?aid=17664021&cid=28837299&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" width="100%" height="500"> </iframe>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;开博第一帖献给我鞠，希望大家可以喜欢上这个努力的女孩子&quot;&gt;&lt;a href=&quot;#开博第一帖献给我鞠，希望大家可以喜欢上这个努力的女孩子&quot; class=&quot;headerlink&quot; title=&quot;开博第一帖献给我鞠，希望大家可以喜欢上这个努力的女孩子&quot;&gt;&lt;/a&gt;开博第一帖
      
    
    </summary>
    
      <category term="Idol" scheme="http://github.com/categories/Idol/"/>
    
    
      <category term="小鞠真好看" scheme="http://github.com/tags/%E5%B0%8F%E9%9E%A0%E7%9C%9F%E5%A5%BD%E7%9C%8B/"/>
    
  </entry>
  
  <entry>
    <title>About Me</title>
    <link href="http://github.com/About_me.html"/>
    <id>http://github.com/About_me.html</id>
    <published>2018-12-15T03:01:28.000Z</published>
    <updated>2019-08-20T01:03:00.294Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Kai-Chen"><a href="#Kai-Chen" class="headerlink" title="Kai Chen"></a>Kai Chen</h1><p>Bsc. @ Fudan University, Computer Science School<br>Interest: Social Network Data Mining / Computer Vision / Object Detection / Machine Learning<br>Email: <a href="mailto:kchen16@fudan.edu.cn" target="_blank" rel="noopener">kchen16@fudan.edu.cn</a><br><a href="/files/CV_of_Kai_Chen.pdf">Download CV</a></p><p><img src="/images/About_me/selfie.jpg" alt="Picture taken in 221B Baker Street"></p><h1 id="Biography"><a href="#Biography" class="headerlink" title="Biography"></a>Biography</h1><p>I’m an undergraduate student majoring in <a href="http://www.cs.fudan.edu.cn/" target="_blank" rel="noopener">Computer Science in Fudan University</a>. I joined in <a href="https://datascience.fudan.edu.cn/" target="_blank" rel="noopener">Shanghai Key Laboratory of Data Science</a> in my sophomore year supervised by <a href="https://datascience.fudan.edu.cn/e1/64/c13398a123236/page.htm" target="_blank" rel="noopener">Prof. Yitong Wang</a> and start my research in machine learning and social network data mining. Focusing on link prediction on Weighted Signed Social Network, we successfully built a new algorithm called MFLG and summarized our work in a paper which is now waiting for notification. You can see details in my CV.</p><p>In my junior year, I was lucky to be an exchange student in <a href="http://www.cs.manchester.ac.uk/" target="_blank" rel="noopener">the University of Manchester</a> from Sep, 2018 to Jan, 2019. And in this year, I start my work in Computer Vision, especially in <strong>VAE and object detection</strong>. I join <a href="http://www.iipl.fudan.edu.cn/" target="_blank" rel="noopener">Shanghai Key Laboratory of Intelligent Information Processing</a> and surpervised by <a href="http://homepage.fudan.edu.cn/binli/" target="_blank" rel="noopener">Dr. Bin Li</a> and <a href="http://homepage.fudan.edu.cn/xyxue/zh" target="_blank" rel="noopener">Prof. Xiangyang Xue</a>. We are focusing on unsupervised object detection algorithm based on VAE / RNN and trying to do this job in more realistic images. </p><p>Besides that, this summer I am so glad to join <a href="https://cs.indiana.edu/~djcran/" target="_blank" rel="noopener">Prof. David Crandall</a> ‘s great <a href="http://vision.soic.indiana.edu/" target="_blank" rel="noopener">vision lab in Indiana University</a>. Looking forward to that! I want to make changes in a closer to daily lives area and I will keep on it.</p><h1 id="Reseach-amp-Project"><a href="#Reseach-amp-Project" class="headerlink" title="Reseach &amp; Project"></a>Reseach &amp; Project</h1><ul><li><p>Unsupervised Object Detection using VAE</p><p>We human can recognize an object in a raw image even if we never see it before because we tend to understand an image in a structural way and decompose the image to many ‘objects’. Based on this idea, we try to find object one at a time and try to find them more precisely.  </p></li><li><p>Link Prediction on Weighted Signed Social Network (WSN)<br>In today’s social network, we have different kinds of attitudes (sign) towards people to different extent (weight). So we try to do link prediction in WSN to get a more precise algorithm. We come up with a new algorithm MFLG and we summarize our work into a paper (under review).</p></li><li><p>P2P chatroom on LAN<br>It’s a basic P2P online chatroom based on OMCS framework. It can achieve communication through text, audio and video.</p></li></ul><h1 id="Honor"><a href="#Honor" class="headerlink" title="Honor"></a>Honor</h1><ul><li>National Scholarship for Outstanding Students (1%)</li><li>Joel &amp; Ruth Spira Scholarship (1%)</li><li>Scholarship for Outstanding Students of Fudan University (10%)</li><li>Outstanding undergraduate of Fudan University (2 times)</li><li>1st Prize - “ChuangQingChun” Enterprising Competition FDU Division</li></ul><h1 id="Interest"><a href="#Interest" class="headerlink" title="Interest"></a>Interest</h1><p>I love basketball and I’m also a big fan of <a href="https://zh.wikipedia.org/wiki/%E6%96%AF%E8%92%82%E8%8A%AC%C2%B7%E7%A7%91%E9%87%8C" target="_blank" rel="noopener">Stepfen Curry</a>, MVP point guard of Golden State Warriors, NBA. I’m a team member of my class’s basketball team and often play score / power forward. In my spare time, I also play the role of a basketball game referee. Hope one day I can have a chance to see a home game of Warriors in San Francisco!</p><p><img src="/images/About_me/1.jpg" style="width:200px float:left"><br><br><img src="/images/About_me/2.jpg" style="width:100%; float:left;margin-left:-6px"><br></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Kai-Chen&quot;&gt;&lt;a href=&quot;#Kai-Chen&quot; class=&quot;headerlink&quot; title=&quot;Kai Chen&quot;&gt;&lt;/a&gt;Kai Chen&lt;/h1&gt;&lt;p&gt;Bsc. @ Fudan University, Computer Science Scho
      
    
    </summary>
    
    
  </entry>
  
</feed>
